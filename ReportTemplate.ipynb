{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO-F-422 -  Statistical Foundations of Machine Learning \n",
    "\n",
    "### Alexandre Flachs - __[alexandre.flachs@ulb.be](mailto:alexandre.flachs@ulb.be) - Student ID 474748__\n",
    "### Marie Giot - __[marie.giot@ulb.be](mailto:marie.giot@ulb.be) - Student ID 474915__\n",
    "### Jeanne Szpirer - __[jeanne.szpirer@ulb.be](mailto:jeanne.szpirer@ulb.be) - Student ID 477286__\n",
    "\n",
    "### Video presentation: www.youtube.com/abcd1234\n",
    "\n",
    "## Flu Shot Learning: Predict H1N1 and Seasonal Flu Vaccines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ajouter du texte d'intro avec jolies images ?* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Before working any model we need to preprocess the data to make it usefull. This pipeline in divided intro three parts :\n",
    "1. **Missing value imputation** : Replace missing values, possibly using other known values\n",
    "2. **Feature engineering** : Define useful features from available ones. \n",
    "3. **Feature selection** : Some features might be useless or give wrong indications to the model, we might need to remove some features.\n",
    "\n",
    "Let's start by importing our data, then develop each of the above parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set features\n",
    "training_set_features <- read.csv(\"training_set_features.csv\", stringsAsFactors = T, na.strings = c(\"NA\", \"\"))\n",
    "dim(training_set_features)\n",
    "\n",
    "# Test set features\n",
    "test_set_features <- read.csv(\"test_set_features.csv\", stringsAsFactors = T, na.strings = c(\"NA\", \"\"))\n",
    "dim(test_set_features)\n",
    "\n",
    "# Training set labels\n",
    "training_set_labels <- read.csv(\"training_set_labels.csv\", stringsAsFactors = T, na.strings = c(\"NA\", \"\"))\n",
    "dim(training_set_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training features set and the training labels set has the same amount of lines, this is a first good sign because it means that we have an \"answer\" for every training line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We summarize our data before doing any work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(training_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have many missing values, in most features. We can compare the number of lines left if we remove any line containing any missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First method\n",
    "cat(\"Training set : \", dim(training_set_features)[1], \"->\", dim(na.omit(training_set_features))[1], \"\\n\")\n",
    "cat(\"Test set     : \", dim(test_set_features)[1], \"->\", dim(na.omit(test_set_features))[1], \"\\n\")\n",
    "cat(\"Training labs: \", dim(training_set_labels)[1], \"->\", dim(na.omit(training_set_labels))[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least no line from the training labels misses any value, we can thus use every entry from the training set for both targets.\n",
    "Counting the number of missing value per feature allows us to see if some of the could be useless. The health insurance, employment occupation and employment industry lines are the emptiest (almost half of the lines miss this data) but by intuition this might be a huge factor in the vaccination decision so we keep it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut aussi regarder si certaines colonnes n'ont vraiment quasi aucune valeur, dans ce cas, Ã§a vaut pas vraiment la peine de garder\n",
    "a <- sapply(training_set_features, function(x) sum(is.na(x)))\n",
    "print(a[order(a, decreasing=T)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we remove these fields ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_nas <- which(sapply(test_set_features, function(x) sum(is.na(x))) > 10000)\n",
    "\n",
    "cat(\"Training set : \",\n",
    "    dim(training_set_features[,-useful_nas])[1], \"->\",\n",
    "    dim(na.omit(training_set_features[,-useful_nas]))[1], \"\\n\")\n",
    "cat(\"Test set     : \", dim(test_set_features[,-useful_nas])[1], \"->\",\n",
    "    dim(na.omit(test_set_features[,-useful_nas]))[1], \"\\n\")\n",
    "cat(\"Training labels: \", dim(training_set_labels)[1], \"->\", dim(na.omit(training_set_labels))[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are much less lines containing missing values. We will thus manage the three most missed fields differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of missing numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation will be the same for training and testing so we merge them\n",
    "features_set <- rbind(training_set_features, test_set_features)\n",
    "\n",
    "\n",
    "# Note the indexes of each dataset to get them back after\n",
    "tr_indexes <- 1:nrow(training_set_features)\n",
    "ts_indexes <- (nrow(training_set_features)+1):(nrow(training_set_features) + nrow(test_set_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer encoding of some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels(features_set[, \"age_group\"])\n",
    "levels(features_set[, \"age_group\"]) <- 0:4\n",
    "features_set[, \"age_group\"] <- as.numeric(features_set[, \"age_group\"])\n",
    "features_set[, \"age_group\"] <- (features_set[, \"age_group\"] - 1)/4\n",
    "\n",
    "levels(features_set[, \"education\"])\n",
    "levels(features_set[, \"education\"]) <- 0:3\n",
    "features_set[, \"education\"] <- as.numeric(features_set[, \"education\"])\n",
    "features_set[, \"education\"] <- (features_set[, \"education\"] - 1)/3\n",
    "\n",
    "levels(features_set[, \"income_poverty\"])\n",
    "levels(features_set[, \"income_poverty\"]) <- c(1, 2, 0)\n",
    "features_set[, \"income_poverty\"] <- as.numeric(features_set[, \"income_poverty\"])\n",
    "features_set[, \"income_poverty\"] <- (features_set[, \"income_poverty\"] - 1)/2\n",
    "\n",
    "levels(features_set[, \"census_msa\"])\n",
    "levels(features_set[, \"census_msa\"]) <- c(1, 2, 0)\n",
    "features_set[, \"census_msa\"] <- as.numeric(features_set[, \"census_msa\"])\n",
    "features_set[, \"census_msa\"] <- (features_set[, \"census_msa\"] - 1)/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non negative matrix factorization based imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non negative matrix factorization is a procedure which consists of approximating a matrix with non negative values as the product of two matrices having smaller dimensions. More formally, a matrix $M$ of dimension $m\\times n$ is decomposed into two matrices $W$ and $H$ of dimensions $m\\times p$ and $p\\times n$ respectively such that $M \\approx WH$.\n",
    "\n",
    "More details concerning the imputation method can be found here https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8510447/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PseudoCode :\n",
    "\n",
    "```pseudocode\n",
    "Consider M of dim m x n having missing values\n",
    "Fix N > 0\n",
    "Normalize M and replace NAs using mean imputation\n",
    "k1 <- floor(max(abs(rank(M) - N/2), 1))\n",
    "kN = min(abs(rX + N/2), m, n)\n",
    "\n",
    "# Compute approximation\n",
    "initialize X array of size kN-k1+1\n",
    "for k in 1...kN-k1+1 do\n",
    "    Compute the nNMF of M on non missing values and store the result in X[k]\n",
    "\n",
    "## Based on all X[k], reconstruct M\n",
    "# Weights based only on non missing values of M\n",
    "initialize d array of size kN-k1+1\n",
    "for k in 1...kN-k1+1 do\n",
    "    Compute d[k] = sum(X_ij - M_ij)/ nb_nonNA\n",
    "\n",
    "# Reconstruct\n",
    "Initialize divisor=0, M_hat of 0 with same dim as M\n",
    "for k in 1...kN-k1+1 do\n",
    "    M_hat += exp(-d[k])*X[k]\n",
    "    denum += exp(-d[k])\n",
    "M_hat <- M_hat/denum\n",
    "```\n",
    "\n",
    "\n",
    "After these steps, M_hat does not contain missing values and the imputation is based on local and global information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(NMFN)\n",
    "\n",
    "# We work only on the columns of numeric type\n",
    "numeric_variables_idx<-which(sapply(features_set[1,],class)!=\"factor\")\n",
    "numeric_variables_idx <- numeric_variables_idx[-c(1, 16)] # Remove resp ID and health insur\n",
    "\n",
    "# We need to normalize this data to use efficient nNMF\n",
    "library(\"caret\")\n",
    "\n",
    "ss <- preProcess(as.data.frame(features_set[, numeric_variables_idx]), method=c(\"range\"))\n",
    "features_set[,numeric_variables_idx] <- predict(ss, as.data.frame(features_set[, numeric_variables_idx]))\n",
    "summary(features_set[, numeric_variables_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that will compute nmf\n",
    "nfm_mult_upd <- function(R, K, missing_idx, maxit=800, eps=2.2204e-16) {\n",
    "    # Using weighted multiplicative rule Zhu 2016\n",
    "    # init random W and H\n",
    "    print(paste(\"[INFO] : NMF with k=\", K))\n",
    "    R <- as.matrix(R)\n",
    "    I <- dim(R)[1]\n",
    "    J <- dim(R)[2]\n",
    "    M <- matrix(1, nrow = dim(R)[1], ncol = dim(R)[2])\n",
    "    M[missing_idx] <- 0\n",
    "    X <- R # Store original R\n",
    "    R <- R*M\n",
    "    W <- matrix(runif(I*K), nrow = I, ncol = K)\n",
    "    H <- matrix(runif(K*J), nrow = K, ncol = J)\n",
    "    \n",
    "    n <- 0\n",
    "    d1 <- 1000\n",
    "    d2 <- 1000\n",
    "    while(n < maxit && !(d1 < eps && d2 < eps)) {\n",
    "        if (n %% 100 == 0) {\n",
    "            print(paste(\"[INFO] : iter\", n, \" Relative error is :\", distance2(X, W%*%H)/distance2(X, R*0)))\n",
    "        }\n",
    "        newH <- H* (t(W) %*% R) / (t(W) %*% W %*% H)\n",
    "        newW <- W*(R %*% t(newH)) / ((W %*% newH) %*% t(newH))\n",
    "        \n",
    "        d1 <- distance2(newH, H)\n",
    "        d2 <- distance2(newW, W)\n",
    "        \n",
    "        H <- newH\n",
    "        W <- newW\n",
    "        n <- n+1\n",
    "    }\n",
    "    \n",
    "    Res <- W%*%H\n",
    "    #Res[missing_idx] <- X[missing_idx]\n",
    "    nmf <- list(\"res\"=Res, \"dst\"=distance2(R, Res)/distance2(R, 0))\n",
    "    return(nmf)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: initialize by defining N and replace NAs by mean\n",
    "N <- 4\n",
    "\n",
    "replace_na_with_mean_value<-function(vec) {\n",
    "    mean_vec <- mean(as.numeric(vec), na.rm=TRUE)\n",
    "    vec[is.na(vec)]<-mean_vec\n",
    "    vec\n",
    "}\n",
    "\n",
    "X<-data.frame(apply(features_set[, numeric_variables_idx], MARGIN=2, replace_na_with_mean_value))\n",
    "miss_idx = which(is.na(features_set[,numeric_variables_idx]), arr.ind = T)\n",
    "non_miss_idx <- which(!is.na(features_set[,numeric_variables_idx]), arr.ind = T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: nNMF for k=k1 to K=kN with masking missing values\n",
    "library(Matrix) # For rankMatrix \n",
    "rX <- rankMatrix(X)\n",
    "k1 = floor(max(abs(rX - N/2), 1))\n",
    "kN = min(abs(rX + N/2), dim(X)[1], dim(X)[2])\n",
    "\n",
    "X_hat = array(0, dim = c(kN-k1+1, nrow(X), ncol(X)))\n",
    "dim(X_hat)\n",
    "for (K in k1:kN) {\n",
    "    # compute NMF\n",
    "    nmf <- nfm_mult_upd(X, K, missing_idx = miss_idx, maxit=800)\n",
    "    print(paste(\"Computed nmf, final dist is\", nmf$dst))\n",
    "    X_hat[K-k1+1,,] <- nmf$res\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: weighted reconstruction\n",
    "d = array(0, dim = c(kN-k1+1))\n",
    "for (K in 1:(kN-k1+1)) {\n",
    "    # Reconstruction error based on non missing values\n",
    "    d[K] <- sum(abs(X_hat[K,,][non_miss_idx] - X[non_miss_idx]))/nrow(non_miss_idx)\n",
    "}\n",
    "\n",
    "X_hat_f <- matrix(0, nrow = nrow(X), ncol = ncol(X))\n",
    "denum <- 0\n",
    "for (K in 1:(kN-k1+1)) {\n",
    "    # Reconstruction matrix\n",
    "    X_hat_f <- X_hat_f + exp(-d[K])*X_hat[K,,]\n",
    "    denum <- denum + exp(-d[K])\n",
    "}\n",
    "X_hat_f <- X_hat_f / sum(exp(-d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace in the feature set\n",
    "X[miss_idx] <- X_hat_f[miss_idx]\n",
    "features_set[, numeric_variables_idx] <- X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we managed numerical variabels we can work on encoding the categorical ones which we did not work on yet.\n",
    "\n",
    "Getting back to the columns containing many missing values, we use one hot encoding and create a category \"missing\", considering that not having this piece of information could be meaning something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_nas <- which(sapply(test_set_features, function(x) sum(is.na(x))) > 10000)\n",
    "\n",
    "library(fastDummies)\n",
    "features_set <- dummy_cols(features_set, \n",
    "                           select_columns = names(useful_nas),\n",
    "                           remove_selected_columns = T)\n",
    "\n",
    "## Change NAs to 0 for the new one hot encoded columns\n",
    "replace_na_with_0<-function(vec) {\n",
    "    vec[is.na(vec)]<-0\n",
    "    vec\n",
    "}\n",
    "\n",
    "useful_nas_feat_idx <- c(grep(\"health_insurance_*\", colnames(features_set)))\n",
    "useful_nas_feat_idx <- c(useful_nas_feat_idx, grep(\"employment_industry*\", colnames(features_set)))\n",
    "useful_nas_feat_idx <- c(useful_nas_feat_idx, grep(\"employment_occup*\", colnames(features_set)))\n",
    "\n",
    "features_set[,useful_nas_feat_idx] <- replace_na_with_0(features_set[,useful_nas_feat_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining columns will be treated the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_variables<-which(sapply(features_set[1,],class)==\"factor\")\n",
    "factor_variables\n",
    "factor_col_names <- colnames(factor_variables)\n",
    "\n",
    "features_set_int_encoded <- dummy_cols(features_set,\n",
    "                                       select_columns = factor_col_names,\n",
    "                                       ignore_na = F,\n",
    "                                       remove_selected_columns = T)\n",
    "\n",
    "\n",
    "features_set_int_encoded <- replace_na_with_0(features_set_int_encoded)\n",
    "\n",
    "tr_set_enc <- features_set_int_encoded[tr_indexes,]\n",
    "ts_set_enc <- features_set_int_encoded[ts_indexes,]\n",
    "\n",
    "summary(features_set_int_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A RETIRER ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose several variables that should influence the output variables in our opinion and according to the scientific articles we have read.\n",
    "### Pas hÃ©siter Ã  les mettre quand on en aura hihi\n",
    "Mais selon moi, \"marital_status\", \"rent_or_own\", \"education\" et \"employment_status\" devraient pas trop influencer. Et \"employment_industry\" & \"employment_occupation\" sont des random short strings hyper nombreux donc pas convaincue que Ã§a soit trÃ¨s utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need dummies package to transform string values with one-hot-encoding.\n",
    "# install.packages('dummies')\n",
    "library(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep only some factor variables\n",
    "variables_to_keep<-c(\"age_group\",\"race\",\"sex\",\"income_poverty\",\"hhs_geo_region\",\"census_msa\")\n",
    "data_factor_onehot <- dummy.data.frame(data_factor[,variables_to_keep], sep=\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(data_factor_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factor_onehot[1:2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed_extended<-cbind(data_preprocessed,data_factor_onehot)\n",
    "dim(data_preprocessed_extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut passer au traitement des missing values. Pour le moment, j'implÃ©mente la mÃªme chose que dans les TP (on remplace par la mean value) mais on peut 100% faire plus de recherches et remplacer par autre chose. J'ai vu que la median value Ã©tait souvent utilisÃ©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_na_with_mean_value<-function(vec) {\n",
    "    mean_vec<-mean(vec,na.rm=T)\n",
    "    vec[is.na(vec)]<-mean_vec\n",
    "    vec\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed_extended<-data.frame(apply(data_preprocessed_extended,2,replace_na_with_mean_value))\n",
    "summary(data_preprocessed_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(na.omit(data_preprocessed_extended))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade, on a un set de data avec seulement des variables numÃ©riques qui ont du sens d'Ãªtre lÃ  (selon moi hihi) et plus de missing values donc il manque surtout feature engineering. On pourra ensuite faire feature selection avec une mesure de la corrÃ©lation entre input et output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "## Model 1\n",
    "We are going to use the simple linear model first to see what kind of accuracy we can get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalement ce sera dans le prÃ©processing mais pas encore donc je laisse les lignes ici.\n",
    "training_features <- read.csv(\"tr_set_imputed.csv\")[, -c(1)]\n",
    "summary(training_features)\n",
    "training_set_labels <- read.csv(\"training_set_labels.csv\", stringsAsFactors = T)\n",
    "label_1 <- training_set_labels[,\"h1n1_vaccine\"]\n",
    "label_2 <- training_set_labels[, \"seasonal_vaccine\"]\n",
    "labels <- cbind(h1n1_vaccine=label_1, seasonal_vaccine=label_2)\n",
    "\n",
    "testing_features <- read.csv(\"ts_set_imputed.csv\")[, -c(1)]\n",
    "feature_set <- rbind(training_features, testing_features)\n",
    "feature_set <- feature_set[, -c(1)] # remove resp id\n",
    "pca <- prcomp(feature_set)\n",
    "summary(pca)\n",
    "\n",
    "pca_feat_set <- pca$x[,1:54]\n",
    "\n",
    "# install.packages(\"caret\")\n",
    "library(\"caret\")\n",
    "\n",
    "ss <- preProcess(as.data.frame(pca_feat_set), method=c(\"range\"))\n",
    "pca_feat_set <- predict(ss, as.data.frame(pca_feat_set))\n",
    "\n",
    "\n",
    "pca_tr_set <- pca_feat_set[1:26707,]\n",
    "pca_ts_set <- pca_feat_set[26708:53415,]\n",
    "total_train_set <- cbind(pca_tr_set, labels)\n",
    "summary(total_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making the prediction for the given test values, it is important to check that the model fits by doing a cross-validation and calculating the AUC and ROC. First, the cross-validation with the multivariate linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "accuracy_vec_h1n1 <- array(0,k)\n",
    "accuracy_vec_seasonal <- array(0,k)\n",
    "threshold <- 0.5\n",
    "\n",
    "# 1. Shuffle the dataset randomly.\n",
    "vaccine_idx <- sample(1:nrow(train_data_set))\n",
    "\n",
    "# 2. Split the dataset into k groups\n",
    "max <- ceiling(nrow(train_data_set)/k)\n",
    "splits <- split(vaccine_idx, ceiling(seq_along(vaccine_idx)/max))\n",
    "\n",
    "# 3. For each unique group:\n",
    "for (i in 1:k){\n",
    "  #3.1 Take the group as a hold out or test data set\n",
    "  test_data <- train_data_set[splits[[i]],]\n",
    "  \n",
    "  #3.2 Take the remaining groups as a training data set\n",
    "  train_data <- train_data_set[-splits[[i]],]\n",
    "  print(paste(\"[INFO] - Training set size:\",dim(train_data)[1],\"- Testing set size\",dim(test_data)[1]))\n",
    "  \n",
    "  #3.3 Fit a model on the training set and evaluate it on the test set\n",
    "  model <- lm(cbind(h1n1_vaccine, seasonal_vaccine) ~ ., data=train_data)\n",
    "  Y_pred <- predict(model,test_data[,-targets])\n",
    "  Y_h1n1 <- test_data[,targets[2]]\n",
    "  Y_seasonal <- test_data[,targets[1]]\n",
    "  \n",
    "  #3.4 Store the prediction of the tree (2 is to take only the P(Y=\"spam\"|x))\n",
    "  Y_hat <- ifelse(Y_pred > threshold,1,0) \n",
    "  # Need one confusion matrix for h1n1 and one for seasonal\n",
    "  confusion_matrix_h1n1 <- table(Y_hat[,1],Y_h1n1)\n",
    "  confusion_matrix_seasonal <- table(Y_hat[,2],Y_seasonal)\n",
    "  \n",
    "  #3.5 Retain the evaluation score and discard the model\n",
    "  accuracy_vec_h1n1[i] = (confusion_matrix_h1n1[1,1]+confusion_matrix_h1n1[2,2])/sum(confusion_matrix_h1n1)\n",
    "  misclassification_rate = 1 - accuracy_vec_h1n1[i]\n",
    "  print(paste(\"[INFO] - Misclassification rate h1n1 -\",i,\"fold:\",misclassification_rate))\n",
    "  \n",
    "  accuracy_vec_seasonal[i] = (confusion_matrix_seasonal[1,1]+confusion_matrix_seasonal[2,2])/sum(confusion_matrix_seasonal)\n",
    "  misclassification_rate = 1 - accuracy_vec_seasonal[i]\n",
    "  print(paste(\"[INFO] - Misclassification rate seasonal -\",i,\"fold:\",misclassification_rate))\n",
    "  \n",
    "}\n",
    "\n",
    "#4. Summarize the skill of the model using the sample of model evaluation scores\n",
    "print(paste(\"[INFO] - Mean misclassification rate h1n1:\",1-mean(accuracy_vec_h1n1)))\n",
    "print(paste(\"[INFO] - Mean misclassification rate seasonal:\",1-mean(accuracy_vec_seasonal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the misclassifaction rate for each of the targets.\n",
    "Then, the ROC and AUC for each of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation of the train in two groups in order to calculate the error between prediction and reality.\n",
    "vaccine_idx <- sample(1:nrow(total_train_set))\n",
    "half_split <- floor(nrow(total_train_set)/2)\n",
    "train_data_set <- total_train_set[vaccine_idx[1:half_split],]\n",
    "test_data <- total_train_set[vaccine_idx[(half_split+1):nrow(total_train_set)],]\n",
    "target_idx <- ncol(train_data_set)\n",
    "targets <- c(target_idx, target_idx-1)\n",
    "\n",
    "# Linear model & prediction\n",
    "model <- lm(cbind(h1n1_vaccine, seasonal_vaccine)~., data=train_data_set)\n",
    "Y_pred <- predict(model,test_data[,-targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC for seasonal vaccine\n",
    "thresholds <- seq(0,0.99,0.05)\n",
    "FPR <- c()\n",
    "TPR <- c()\n",
    "\n",
    "for(threshold in thresholds){\n",
    "  Y_hat <- ifelse(Y_pred > threshold,1,0) \n",
    "  confusion_matrix <- table(Y_hat[,2],test_data[,targets[1]])\n",
    "  \n",
    "  if(dim(confusion_matrix)[1] < 2){ \n",
    "    if(rownames(confusion_matrix) == 0){\n",
    "      confusion_matrix <- rbind(confusion_matrix,c(0,0))\n",
    "      rownames(confusion_matrix)[2] <- 1\n",
    "    }\n",
    "    if(rownames(confusion_matrix) == 1){\n",
    "      confusion_matrix <- rbind(c(0,0),confusion_matrix)\n",
    "      rownames(confusion_matrix)[1] <- 0\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  FP <- confusion_matrix[2,1]\n",
    "  TP <- confusion_matrix[2,2]\n",
    "  N_N <- sum(confusion_matrix[,1]) # Total number of 0's\n",
    "  N_P <- sum(confusion_matrix[,2]) # Total number of 1's\n",
    "  \n",
    "  FPR <- c(FPR,FP/N_N)\n",
    "  TPR <- c(TPR,TP/N_P)\n",
    "}\n",
    "\n",
    "plot.new()\n",
    "plot(FPR,TPR)\n",
    "lines(FPR,TPR,col=\"blue\")\n",
    "lines(thresholds,thresholds,lty=2)\n",
    "title(\"ROC Curve for seasonal vaccine\")\n",
    "AUC <- sum(abs(diff(FPR)) * (head(TPR,-1)+tail(TPR,-1)))/2\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC for h1n1 vaccine\n",
    "thresholds <- seq(0,0.99,0.05)\n",
    "FPR <- c()\n",
    "TPR <- c()\n",
    "\n",
    "for(threshold in thresholds){\n",
    "  Y_hat <- ifelse(Y_pred > threshold,1,0) \n",
    "  confusion_matrix <- table(Y_hat[,1],test_data[,targets[2]])\n",
    "  \n",
    "  if(dim(confusion_matrix)[1] < 2){ \n",
    "    if(rownames(confusion_matrix) == 0){\n",
    "      confusion_matrix <- rbind(confusion_matrix,c(0,0))\n",
    "      rownames(confusion_matrix)[2] <- 1\n",
    "    }\n",
    "    if(rownames(confusion_matrix) == 1){\n",
    "      confusion_matrix <- rbind(c(0,0),confusion_matrix)\n",
    "      rownames(confusion_matrix)[1] <- 0\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  FP <- confusion_matrix[2,1]\n",
    "  TP <- confusion_matrix[2,2]\n",
    "  N_N <- sum(confusion_matrix[,1]) # Total number of 0's\n",
    "  N_P <- sum(confusion_matrix[,2]) # Total number of 1's\n",
    "  \n",
    "  FPR <- c(FPR,FP/N_N)\n",
    "  TPR <- c(TPR,TP/N_P)\n",
    "}\n",
    "AUC <- sum(abs(diff(FPR)) * (head(TPR,-1)+tail(TPR,-1)))/2\n",
    "AUC\n",
    "\n",
    "plot.new()\n",
    "plot(FPR,TPR)\n",
    "lines(FPR,TPR,col=\"blue\")\n",
    "lines(thresholds,thresholds,lty=2)\n",
    "title(\"ROC Curve for h1n1 vaccine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the linear model is validated, it can be used to predict the outcome with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- lm(cbind(h1n1_vaccine, seasonal_vaccine)~., data=total_train_set)\n",
    "Y_pred <- predict(model,pca_ts_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "Then we use the randomforest package to make another model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"randomForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"randomForest\")\n",
    "n_trees <- 5\n",
    "n_trees\n",
    "accuracy_vec <- array(0,n_trees)\n",
    "accuracy_vec\n",
    "\n",
    "vaccine_idx <- sample(1:nrow(total_train_set))\n",
    "half_split <- floor(nrow(total_train_set)/2)\n",
    "half_split\n",
    "\n",
    "for (i in 1:n_trees){ print(i)\n",
    "    #3.1 Take the first half of the dataset as a training data set\n",
    "    train_data <- total_train_set[vaccine_idx[1:half_split],]\n",
    "\n",
    "    #3.2 Take the second half of the dataset as a hold out or test data set\n",
    "    test_data <- total_train_set[vaccine_idx[(half_split+1):nrow(total_train_set)],]\n",
    "    \n",
    "    model <- randomForest(x=train_data[,-c(labels)],\n",
    "                          y=as.factor(train_data[,c(labels)]),\n",
    "                          xtest=test_data[,-c(labels)],\n",
    "                          ytest=as.factor(test_data[,c(labels)]),\n",
    "                          ntree=i)\n",
    "    \n",
    "    accuracy_vec[i] = (model$test$confusion[1,1]+model$test$confusion[2,2])/sum(model$test$confusion)\n",
    "    }\n",
    "\n",
    "plot(accuracy_vec,main = \"Number of trees influence\",xlab = \"Nbr of trees\",ylab = \"Classification rate\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne pas dÃ©commenter car pas sÃ»re que Ã§a fonctionne\n",
    "#library(\"randomForest\")\n",
    "# Create the forest.\n",
    "#output.forest <- randomForest(formula=h1n1_vaccine+seasonal_vaccine~., data=total_train_set)\n",
    "\n",
    "# View the forest results.\n",
    "#print(output.forest) \n",
    "\n",
    "# Importance of each predictor.\n",
    "#print(importance(fit,type = 2)) \n",
    "#help randomForest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "\n",
    "#### Example of simple equation\n",
    "\\begin{equation}\n",
    "e = mc^2\n",
    "\\end{equation}\n",
    "\n",
    "#### Example of matrix equation - Cross product formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{V}_1 \\times \\mathbf{V}_2 =  \\begin{vmatrix}\n",
    "\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n",
    "\\frac{\\partial X}{\\partial u} &  \\frac{\\partial Y}{\\partial u} & 0 \\\\\n",
    "\\frac{\\partial X}{\\partial v} &  \\frac{\\partial Y}{\\partial v} & 0\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "#### Example of multiline equation - The Lorenz Equations:\n",
    "\n",
    "\\begin{align}\n",
    "\\dot{x} & = \\sigma(y-x) \\\\\n",
    "\\dot{y} & = \\rho x - y - xz \\\\\n",
    "\\dot{z} & = -\\beta z + xy\n",
    "\\end{align}\n",
    "\n",
    "#### Example of Markdown Table:\n",
    "\n",
    "| This | is   |\n",
    "|------|------|\n",
    "|   a  | table|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative models\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
