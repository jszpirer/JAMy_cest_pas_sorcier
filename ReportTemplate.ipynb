{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO-F-422 -  Statistical Foundations of Machine Learning \n",
    "\n",
    "### Alexandre Flachs - __[alexandre.flachs@ulb.be](mailto:alexandre.flachs@ulb.be) - Student ID 474748__\n",
    "### Marie Giot - __[marie.giot@ulb.be](mailto:marie.giot@ulb.be) - Student ID 474915__\n",
    "### Jeanne Szpirer - __[jeanne.szpirer@ulb.be](mailto:jeanne.szpirer@ulb.be) - Student ID 477286__\n",
    "\n",
    "### Video presentation: www.youtube.com/abcd1234\n",
    "\n",
    "## Flu Shot Learning: Predict H1N1 and Seasonal Flu Vaccines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project's goal is to propose efficient machine learning methods to predict the likeliness of patients to get a vaccine for H1N1 or Seasonal Flu. This challenge was inititated by the \"Driven Data\" plateforme. Four different techniques are presented hereunder and their costs and their results will be discussed. The group also submitted its result on the Driven Data plateforme for the challenge in order to receive a score and compare it to the other paticipants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Before working any model we need to preprocess the data to make it usefull. This pipeline in divided intro three parts :\n",
    "1. **Missing value imputation** : Replace missing values, possibly using other known values\n",
    "2. **Feature engineering** : Define useful features from available ones. \n",
    "3. **Feature selection** : Some features might be useless or give wrong indications to the model, we might need to remove some features.\n",
    "\n",
    "Let's start by importing our data, then develop each of the above parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>26707</li><li>36</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 26707\n",
       "\\item 36\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 26707\n",
       "2. 36\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 26707    36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>26708</li><li>36</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 26708\n",
       "\\item 36\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 26708\n",
       "2. 36\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 26708    36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>26707</li><li>3</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 26707\n",
       "\\item 3\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 26707\n",
       "2. 3\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 26707     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training set features\n",
    "training_set_features <- read.csv(\"training_set_features.csv\", stringsAsFactors = T, na.strings = c(\"NA\", \"\"))\n",
    "dim(training_set_features)\n",
    "\n",
    "# Test set features\n",
    "test_set_features <- read.csv(\"test_set_features.csv\", stringsAsFactors = T, na.strings = c(\"NA\", \"\"))\n",
    "dim(test_set_features)\n",
    "\n",
    "# Training set labels\n",
    "training_set_labels <- read.csv(\"training_set_labels.csv\", stringsAsFactors = T, na.strings = c(\"NA\", \"\"))\n",
    "dim(training_set_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training features set and the training labels set has the same amount of lines, this is a first good sign because it means that we have an \"answer\" for every training line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We summarize our data before doing any work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " respondent_id    h1n1_concern   h1n1_knowledge  behavioral_antiviral_meds\n",
       " Min.   :    0   Min.   :0.000   Min.   :0.000   Min.   :0.00000          \n",
       " 1st Qu.: 6676   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:0.00000          \n",
       " Median :13353   Median :2.000   Median :1.000   Median :0.00000          \n",
       " Mean   :13353   Mean   :1.618   Mean   :1.263   Mean   :0.04884          \n",
       " 3rd Qu.:20030   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:0.00000          \n",
       " Max.   :26706   Max.   :3.000   Max.   :2.000   Max.   :1.00000          \n",
       "                 NA's   :92      NA's   :116     NA's   :71               \n",
       " behavioral_avoidance behavioral_face_mask behavioral_wash_hands\n",
       " Min.   :0.0000       Min.   :0.00000      Min.   :0.0000       \n",
       " 1st Qu.:0.0000       1st Qu.:0.00000      1st Qu.:1.0000       \n",
       " Median :1.0000       Median :0.00000      Median :1.0000       \n",
       " Mean   :0.7256       Mean   :0.06898      Mean   :0.8256       \n",
       " 3rd Qu.:1.0000       3rd Qu.:0.00000      3rd Qu.:1.0000       \n",
       " Max.   :1.0000       Max.   :1.00000      Max.   :1.0000       \n",
       " NA's   :208          NA's   :19           NA's   :42           \n",
       " behavioral_large_gatherings behavioral_outside_home behavioral_touch_face\n",
       " Min.   :0.0000              Min.   :0.0000          Min.   :0.0000       \n",
       " 1st Qu.:0.0000              1st Qu.:0.0000          1st Qu.:0.0000       \n",
       " Median :0.0000              Median :0.0000          Median :1.0000       \n",
       " Mean   :0.3586              Mean   :0.3373          Mean   :0.6773       \n",
       " 3rd Qu.:1.0000              3rd Qu.:1.0000          3rd Qu.:1.0000       \n",
       " Max.   :1.0000              Max.   :1.0000          Max.   :1.0000       \n",
       " NA's   :87                  NA's   :82              NA's   :128          \n",
       " doctor_recc_h1n1 doctor_recc_seasonal chronic_med_condition\n",
       " Min.   :0.0000   Min.   :0.0000       Min.   :0.0000       \n",
       " 1st Qu.:0.0000   1st Qu.:0.0000       1st Qu.:0.0000       \n",
       " Median :0.0000   Median :0.0000       Median :0.0000       \n",
       " Mean   :0.2203   Mean   :0.3297       Mean   :0.2833       \n",
       " 3rd Qu.:0.0000   3rd Qu.:1.0000       3rd Qu.:1.0000       \n",
       " Max.   :1.0000   Max.   :1.0000       Max.   :1.0000       \n",
       " NA's   :2160     NA's   :2160         NA's   :971          \n",
       " child_under_6_months health_worker    health_insurance\n",
       " Min.   :0.0000       Min.   :0.0000   Min.   :0.00    \n",
       " 1st Qu.:0.0000       1st Qu.:0.0000   1st Qu.:1.00    \n",
       " Median :0.0000       Median :0.0000   Median :1.00    \n",
       " Mean   :0.0826       Mean   :0.1119   Mean   :0.88    \n",
       " 3rd Qu.:0.0000       3rd Qu.:0.0000   3rd Qu.:1.00    \n",
       " Max.   :1.0000       Max.   :1.0000   Max.   :1.00    \n",
       " NA's   :820          NA's   :804      NA's   :12274   \n",
       " opinion_h1n1_vacc_effective opinion_h1n1_risk opinion_h1n1_sick_from_vacc\n",
       " Min.   :1.000               Min.   :1.000     Min.   :1.000              \n",
       " 1st Qu.:3.000               1st Qu.:1.000     1st Qu.:1.000              \n",
       " Median :4.000               Median :2.000     Median :2.000              \n",
       " Mean   :3.851               Mean   :2.343     Mean   :2.358              \n",
       " 3rd Qu.:5.000               3rd Qu.:4.000     3rd Qu.:4.000              \n",
       " Max.   :5.000               Max.   :5.000     Max.   :5.000              \n",
       " NA's   :391                 NA's   :388       NA's   :395                \n",
       " opinion_seas_vacc_effective opinion_seas_risk opinion_seas_sick_from_vacc\n",
       " Min.   :1.000               Min.   :1.000     Min.   :1.000              \n",
       " 1st Qu.:4.000               1st Qu.:2.000     1st Qu.:1.000              \n",
       " Median :4.000               Median :2.000     Median :2.000              \n",
       " Mean   :4.026               Mean   :2.719     Mean   :2.118              \n",
       " 3rd Qu.:5.000               3rd Qu.:4.000     3rd Qu.:4.000              \n",
       " Max.   :5.000               Max.   :5.000     Max.   :5.000              \n",
       " NA's   :462                 NA's   :514       NA's   :537                \n",
       "         age_group               education                    race      \n",
       " 18 - 34 Years:5215   < 12 Years      : 2363   Black            : 2118  \n",
       " 35 - 44 Years:3848   12 Years        : 5797   Hispanic         : 1755  \n",
       " 45 - 54 Years:5238   College Graduate:10097   Other or Multiple: 1612  \n",
       " 55 - 64 Years:5563   Some College    : 7043   White            :21222  \n",
       " 65+ Years    :6843   NA's            : 1407                            \n",
       "                                                                        \n",
       "                                                                        \n",
       "     sex                          income_poverty      marital_status \n",
       " Female:15858   <= $75,000, Above Poverty:12777   Married    :13555  \n",
       " Male  :10849   > $75,000                : 6810   Not Married:11744  \n",
       "                Below Poverty            : 2697   NA's       : 1408  \n",
       "                NA's                     : 4423                      \n",
       "                                                                     \n",
       "                                                                     \n",
       "                                                                     \n",
       " rent_or_own           employment_status  hhs_geo_region\n",
       " Own :18736   Employed          :13560   lzgpxyit:4297  \n",
       " Rent: 5929   Not in Labor Force:10231   fpwskwrf:3265  \n",
       " NA's: 2042   Unemployed        : 1453   qufhixun:3102  \n",
       "              NA's              : 1463   oxchjgsf:2859  \n",
       "                                         kbazzjca:2858  \n",
       "                                         bhuqouqj:2846  \n",
       "                                         (Other) :7480  \n",
       "                    census_msa    household_adults household_children\n",
       " MSA, Not Principle  City:11645   Min.   :0.0000   Min.   :0.0000    \n",
       " MSA, Principle City     : 7864   1st Qu.:0.0000   1st Qu.:0.0000    \n",
       " Non-MSA                 : 7198   Median :1.0000   Median :0.0000    \n",
       "                                  Mean   :0.8865   Mean   :0.5346    \n",
       "                                  3rd Qu.:1.0000   3rd Qu.:1.0000    \n",
       "                                  Max.   :3.0000   Max.   :3.0000    \n",
       "                                  NA's   :249      NA's   :249       \n",
       " employment_industry employment_occupation\n",
       " fcxhlnwr: 2468      xtkaffoo: 1778       \n",
       " wxleyezf: 1804      mxkfnird: 1509       \n",
       " ldnlellj: 1231      emcorrxb: 1270       \n",
       " pxcmvdjn: 1037      cmhcxjea: 1247       \n",
       " atmlpfrs:  926      xgwztkwe: 1082       \n",
       " (Other) : 5911      (Other) : 6351       \n",
       " NA's    :13330      NA's    :13470       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(training_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have many missing values, in most features. We can compare the number of lines left if we remove any line containing any missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set :  26707 -> 6437 \n",
      "Test set     :  26708 -> 6499 \n",
      "Training labs:  26707 -> 26707"
     ]
    }
   ],
   "source": [
    "# First method\n",
    "cat(\"Training set : \", dim(training_set_features)[1], \"->\", dim(na.omit(training_set_features))[1], \"\\n\")\n",
    "cat(\"Test set     : \", dim(test_set_features)[1], \"->\", dim(na.omit(test_set_features))[1], \"\\n\")\n",
    "cat(\"Training labs: \", dim(training_set_labels)[1], \"->\", dim(na.omit(training_set_labels))[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least no line from the training labels misses any value, we can thus use every entry from the training set for both targets.\n",
    "Counting the number of missing value per feature allows us to see if some of the could be useless. The health insurance, employment occupation and employment industry lines are the emptiest (almost half of the lines miss this data) but by intuition this might be a huge factor in the vaccination decision so we keep it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      employment_occupation         employment_industry \n",
      "                      13470                       13330 \n",
      "           health_insurance              income_poverty \n",
      "                      12274                        4423 \n",
      "           doctor_recc_h1n1        doctor_recc_seasonal \n",
      "                       2160                        2160 \n",
      "                rent_or_own           employment_status \n",
      "                       2042                        1463 \n",
      "             marital_status                   education \n",
      "                       1408                        1407 \n",
      "      chronic_med_condition        child_under_6_months \n",
      "                        971                         820 \n",
      "              health_worker opinion_seas_sick_from_vacc \n",
      "                        804                         537 \n",
      "          opinion_seas_risk opinion_seas_vacc_effective \n",
      "                        514                         462 \n",
      "opinion_h1n1_sick_from_vacc opinion_h1n1_vacc_effective \n",
      "                        395                         391 \n",
      "          opinion_h1n1_risk            household_adults \n",
      "                        388                         249 \n",
      "         household_children        behavioral_avoidance \n",
      "                        249                         208 \n",
      "      behavioral_touch_face              h1n1_knowledge \n",
      "                        128                         116 \n",
      "               h1n1_concern behavioral_large_gatherings \n",
      "                         92                          87 \n",
      "    behavioral_outside_home   behavioral_antiviral_meds \n",
      "                         82                          71 \n",
      "      behavioral_wash_hands        behavioral_face_mask \n",
      "                         42                          19 \n",
      "              respondent_id                   age_group \n",
      "                          0                           0 \n",
      "                       race                         sex \n",
      "                          0                           0 \n",
      "             hhs_geo_region                  census_msa \n",
      "                          0                           0 \n"
     ]
    }
   ],
   "source": [
    "# On peut aussi regarder si certaines colonnes n'ont vraiment quasi aucune valeur, dans ce cas, ça vaut pas vraiment la peine de garder\n",
    "a <- sapply(training_set_features, function(x) sum(is.na(x)))\n",
    "print(a[order(a, decreasing=T)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we remove these fields ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set :  26707 -> 19642 \n",
      "Test set     :  26708 -> 19592 \n",
      "Training labels:  26707 -> 26707"
     ]
    }
   ],
   "source": [
    "useful_nas <- which(sapply(test_set_features, function(x) sum(is.na(x))) > 10000)\n",
    "\n",
    "cat(\"Training set : \",\n",
    "    dim(training_set_features[,-useful_nas])[1], \"->\",\n",
    "    dim(na.omit(training_set_features[,-useful_nas]))[1], \"\\n\")\n",
    "cat(\"Test set     : \", dim(test_set_features[,-useful_nas])[1], \"->\",\n",
    "    dim(na.omit(test_set_features[,-useful_nas]))[1], \"\\n\")\n",
    "cat(\"Training labels: \", dim(training_set_labels)[1], \"->\", dim(na.omit(training_set_labels))[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are much less lines containing missing values. We will thus manage the three most missed fields differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of missing numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation will be the same for training and testing so we merge them\n",
    "features_set <- rbind(training_set_features, test_set_features)\n",
    "\n",
    "\n",
    "# Note the indexes of each dataset to get them back after\n",
    "tr_indexes <- 1:nrow(training_set_features)\n",
    "ts_indexes <- (nrow(training_set_features)+1):(nrow(training_set_features) + nrow(test_set_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer encoding of some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'18 - 34 Years'</li><li>'35 - 44 Years'</li><li>'45 - 54 Years'</li><li>'55 - 64 Years'</li><li>'65+ Years'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '18 - 34 Years'\n",
       "\\item '35 - 44 Years'\n",
       "\\item '45 - 54 Years'\n",
       "\\item '55 - 64 Years'\n",
       "\\item '65+ Years'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '18 - 34 Years'\n",
       "2. '35 - 44 Years'\n",
       "3. '45 - 54 Years'\n",
       "4. '55 - 64 Years'\n",
       "5. '65+ Years'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"18 - 34 Years\" \"35 - 44 Years\" \"45 - 54 Years\" \"55 - 64 Years\"\n",
       "[5] \"65+ Years\"    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'&lt; 12 Years'</li><li>'12 Years'</li><li>'College Graduate'</li><li>'Some College'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '< 12 Years'\n",
       "\\item '12 Years'\n",
       "\\item 'College Graduate'\n",
       "\\item 'Some College'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '&lt; 12 Years'\n",
       "2. '12 Years'\n",
       "3. 'College Graduate'\n",
       "4. 'Some College'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"< 12 Years\"       \"12 Years\"         \"College Graduate\" \"Some College\"    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'&lt;= $75,000, Above Poverty'</li><li>'&gt; $75,000'</li><li>'Below Poverty'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '<= \\$75,000, Above Poverty'\n",
       "\\item '> \\$75,000'\n",
       "\\item 'Below Poverty'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '&lt;= $75,000, Above Poverty'\n",
       "2. '&gt; $75,000'\n",
       "3. 'Below Poverty'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"<= $75,000, Above Poverty\" \"> $75,000\"                \n",
       "[3] \"Below Poverty\"            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li><span style=white-space:pre-wrap>'MSA, Not Principle  City'</span></li><li>'MSA, Principle City'</li><li>'Non-MSA'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'MSA, Not Principle  City'\n",
       "\\item 'MSA, Principle City'\n",
       "\\item 'Non-MSA'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. <span style=white-space:pre-wrap>'MSA, Not Principle  City'</span>\n",
       "2. 'MSA, Principle City'\n",
       "3. 'Non-MSA'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"MSA, Not Principle  City\" \"MSA, Principle City\"     \n",
       "[3] \"Non-MSA\"                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "levels(features_set[, \"age_group\"])\n",
    "levels(features_set[, \"age_group\"]) <- 0:4\n",
    "features_set[, \"age_group\"] <- as.numeric(features_set[, \"age_group\"])\n",
    "features_set[, \"age_group\"] <- (features_set[, \"age_group\"] - 1)/4\n",
    "\n",
    "levels(features_set[, \"education\"])\n",
    "levels(features_set[, \"education\"]) <- 0:3\n",
    "features_set[, \"education\"] <- as.numeric(features_set[, \"education\"])\n",
    "features_set[, \"education\"] <- (features_set[, \"education\"] - 1)/3\n",
    "\n",
    "levels(features_set[, \"income_poverty\"])\n",
    "levels(features_set[, \"income_poverty\"]) <- c(1, 2, 0)\n",
    "features_set[, \"income_poverty\"] <- as.numeric(features_set[, \"income_poverty\"])\n",
    "features_set[, \"income_poverty\"] <- (features_set[, \"income_poverty\"] - 1)/2\n",
    "\n",
    "levels(features_set[, \"census_msa\"])\n",
    "levels(features_set[, \"census_msa\"]) <- c(1, 2, 0)\n",
    "features_set[, \"census_msa\"] <- as.numeric(features_set[, \"census_msa\"])\n",
    "features_set[, \"census_msa\"] <- (features_set[, \"census_msa\"] - 1)/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non negative matrix factorization based imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non negative matrix factorization is a procedure which consists of approximating a matrix with non negative values as the product of two matrices having smaller dimensions. More formally, a matrix $M$ of dimension $m\\times n$ is decomposed into two matrices $W$ and $H$ of dimensions $m\\times p$ and $p\\times n$ respectively such that $M \\approx WH$.\n",
    "\n",
    "More details concerning the imputation method can be found here https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8510447/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PseudoCode :\n",
    "\n",
    "```pseudocode\n",
    "Consider M of dim m x n having missing values\n",
    "Fix N > 0\n",
    "Normalize M and replace NAs using mean imputation\n",
    "k1 <- floor(max(abs(rank(M) - N/2), 1))\n",
    "kN = min(abs(rX + N/2), m, n)\n",
    "\n",
    "# Compute approximation\n",
    "initialize X array of size kN-k1+1\n",
    "for k in 1...kN-k1+1 do\n",
    "    Compute the nNMF of M on non missing values and store the result in X[k]\n",
    "\n",
    "## Based on all X[k], reconstruct M\n",
    "# Weights based only on non missing values of M\n",
    "initialize d array of size kN-k1+1\n",
    "for k in 1...kN-k1+1 do\n",
    "    Compute d[k] = sum(X_ij - M_ij)/ nb_nonNA\n",
    "\n",
    "# Reconstruct\n",
    "Initialize divisor=0, M_hat of 0 with same dim as M\n",
    "for k in 1...kN-k1+1 do\n",
    "    M_hat += exp(-d[k])*X[k]\n",
    "    denum += exp(-d[k])\n",
    "M_hat <- M_hat/denum\n",
    "```\n",
    "\n",
    "\n",
    "After these steps, M_hat does not contain missing values and the imputation is based on local and global information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: ggplot2\n",
      "\n",
      "Loading required package: lattice\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  h1n1_concern    h1n1_knowledge   behavioral_antiviral_meds\n",
       " Min.   :0.0000   Min.   :0.0000   Min.   :0.00000          \n",
       " 1st Qu.:0.3333   1st Qu.:0.5000   1st Qu.:0.00000          \n",
       " Median :0.6667   Median :0.5000   Median :0.00000          \n",
       " Mean   :0.5403   Mean   :0.6321   Mean   :0.04924          \n",
       " 3rd Qu.:0.6667   3rd Qu.:1.0000   3rd Qu.:0.00000          \n",
       " Max.   :1.0000   Max.   :1.0000   Max.   :1.00000          \n",
       " NA's   :177      NA's   :238      NA's   :150              \n",
       " behavioral_avoidance behavioral_face_mask behavioral_wash_hands\n",
       " Min.   :0.0000       Min.   :0.00000      Min.   :0.0000       \n",
       " 1st Qu.:0.0000       1st Qu.:0.00000      1st Qu.:1.0000       \n",
       " Median :1.0000       Median :0.00000      Median :1.0000       \n",
       " Mean   :0.7277       Mean   :0.06913      Mean   :0.8258       \n",
       " 3rd Qu.:1.0000       3rd Qu.:0.00000      3rd Qu.:1.0000       \n",
       " Max.   :1.0000       Max.   :1.00000      Max.   :1.0000       \n",
       " NA's   :421          NA's   :38           NA's   :82           \n",
       " behavioral_large_gatherings behavioral_outside_home behavioral_touch_face\n",
       " Min.   :0.0000              Min.   :0.0000          Min.   :0.0000       \n",
       " 1st Qu.:0.0000              1st Qu.:0.0000          1st Qu.:0.0000       \n",
       " Median :0.0000              Median :0.0000          Median :1.0000       \n",
       " Mean   :0.3551              Mean   :0.3373          Mean   :0.6805       \n",
       " 3rd Qu.:1.0000              3rd Qu.:1.0000          3rd Qu.:1.0000       \n",
       " Max.   :1.0000              Max.   :1.0000          Max.   :1.0000       \n",
       " NA's   :159                 NA's   :164             NA's   :256          \n",
       " doctor_recc_h1n1 doctor_recc_seasonal chronic_med_condition\n",
       " Min.   :0.000    Min.   :0.000        Min.   :0.0000       \n",
       " 1st Qu.:0.000    1st Qu.:0.000        1st Qu.:0.0000       \n",
       " Median :0.000    Median :0.000        Median :0.0000       \n",
       " Mean   :0.221    Mean   :0.332        Mean   :0.2821       \n",
       " 3rd Qu.:0.000    3rd Qu.:1.000        3rd Qu.:1.0000       \n",
       " Max.   :1.000    Max.   :1.000        Max.   :1.0000       \n",
       " NA's   :4320     NA's   :4320         NA's   :1903         \n",
       " child_under_6_months health_worker    opinion_h1n1_vacc_effective\n",
       " Min.   :0.0000       Min.   :0.0000   Min.   :0.0000             \n",
       " 1st Qu.:0.0000       1st Qu.:0.0000   1st Qu.:0.5000             \n",
       " Median :0.0000       Median :0.0000   Median :0.7500             \n",
       " Mean   :0.0845       Mean   :0.1117   Mean   :0.7119             \n",
       " 3rd Qu.:0.0000       3rd Qu.:0.0000   3rd Qu.:1.0000             \n",
       " Max.   :1.0000       Max.   :1.0000   Max.   :1.0000             \n",
       " NA's   :1633         NA's   :1593     NA's   :789                \n",
       " opinion_h1n1_risk opinion_h1n1_sick_from_vacc opinion_seas_vacc_effective\n",
       " Min.   :0.0000    Min.   :0.0000              Min.   :0.0000             \n",
       " 1st Qu.:0.0000    1st Qu.:0.0000              1st Qu.:0.7500             \n",
       " Median :0.2500    Median :0.2500              Median :0.7500             \n",
       " Mean   :0.3337    Mean   :0.3398              Mean   :0.7564             \n",
       " 3rd Qu.:0.7500    3rd Qu.:0.7500              3rd Qu.:1.0000             \n",
       " Max.   :1.0000    Max.   :1.0000              Max.   :1.0000             \n",
       " NA's   :768       NA's   :770                 NA's   :914                \n",
       " opinion_seas_risk opinion_seas_sick_from_vacc   age_group     \n",
       " Min.   :0.0000    Min.   :0.0000              Min.   :0.0000  \n",
       " 1st Qu.:0.2500    1st Qu.:0.0000              1st Qu.:0.2500  \n",
       " Median :0.2500    Median :0.2500              Median :0.5000  \n",
       " Mean   :0.4285    Mean   :0.2827              Mean   :0.5445  \n",
       " 3rd Qu.:0.7500    3rd Qu.:0.7500              3rd Qu.:1.0000  \n",
       " Max.   :1.0000    Max.   :1.0000              Max.   :1.0000  \n",
       " NA's   :1013      NA's   :1058                                \n",
       "   education      income_poverty    census_msa    household_adults\n",
       " Min.   :0.0000   Min.   :0.000   Min.   :0.000   Min.   :0.0000  \n",
       " 1st Qu.:0.3333   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.0000  \n",
       " Median :0.6667   Median :0.000   Median :0.500   Median :0.3333  \n",
       " Mean   :0.6221   Mean   :0.272   Mean   :0.412   Mean   :0.2968  \n",
       " 3rd Qu.:1.0000   3rd Qu.:0.500   3rd Qu.:1.000   3rd Qu.:0.3333  \n",
       " Max.   :1.0000   Max.   :1.000   Max.   :1.000   Max.   :1.0000  \n",
       " NA's   :2814     NA's   :8920                    NA's   :474     \n",
       " household_children\n",
       " Min.   :0.0000    \n",
       " 1st Qu.:0.0000    \n",
       " Median :0.0000    \n",
       " Mean   :0.1797    \n",
       " 3rd Qu.:0.3333    \n",
       " Max.   :1.0000    \n",
       " NA's   :474       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(NMFN)\n",
    "\n",
    "# We work only on the columns of numeric type\n",
    "numeric_variables_idx<-which(sapply(features_set[1,],class)!=\"factor\")\n",
    "numeric_variables_idx <- numeric_variables_idx[-c(1, 16)] # Remove resp ID and health insur\n",
    "\n",
    "# We need to normalize this data to use efficient nNMF\n",
    "library(\"caret\")\n",
    "\n",
    "ss <- preProcess(as.data.frame(features_set[, numeric_variables_idx]), method=c(\"range\"))\n",
    "features_set[,numeric_variables_idx] <- predict(ss, as.data.frame(features_set[, numeric_variables_idx]))\n",
    "summary(features_set[, numeric_variables_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that will compute nmf\n",
    "nfm_mult_upd <- function(R, K, missing_idx, maxit=800, eps=2.2204e-16) {\n",
    "    # Using weighted multiplicative rule Zhu 2016\n",
    "    # init random W and H\n",
    "    print(paste(\"[INFO] : NMF with k=\", K))\n",
    "    R <- as.matrix(R)\n",
    "    I <- dim(R)[1]\n",
    "    J <- dim(R)[2]\n",
    "    M <- matrix(1, nrow = dim(R)[1], ncol = dim(R)[2])\n",
    "    M[missing_idx] <- 0\n",
    "    X <- R # Store original R\n",
    "    R <- R*M\n",
    "    W <- matrix(runif(I*K), nrow = I, ncol = K)\n",
    "    H <- matrix(runif(K*J), nrow = K, ncol = J)\n",
    "    \n",
    "    n <- 0\n",
    "    d1 <- 1000\n",
    "    d2 <- 1000\n",
    "    while(n < maxit && !(d1 < eps && d2 < eps)) {\n",
    "        if (n %% 100 == 0) {\n",
    "            print(paste(\"[INFO] : iter\", n, \" Relative error is :\", distance2(X, W%*%H)/distance2(X, R*0)))\n",
    "        }\n",
    "        newH <- H* (t(W) %*% R) / (t(W) %*% W %*% H)\n",
    "        newW <- W*(R %*% t(newH)) / ((W %*% newH) %*% t(newH))\n",
    "        \n",
    "        d1 <- distance2(newH, H)\n",
    "        d2 <- distance2(newW, W)\n",
    "        \n",
    "        H <- newH\n",
    "        W <- newW\n",
    "        n <- n+1\n",
    "    }\n",
    "    \n",
    "    Res <- W%*%H\n",
    "    #Res[missing_idx] <- X[missing_idx]\n",
    "    nmf <- list(\"res\"=Res, \"dst\"=distance2(R, Res)/distance2(R, 0))\n",
    "    return(nmf)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: initialize by defining N and replace NAs by mean\n",
    "N <- 4\n",
    "\n",
    "replace_na_with_mean_value<-function(vec) {\n",
    "    mean_vec <- mean(as.numeric(vec), na.rm=TRUE)\n",
    "    vec[is.na(vec)]<-mean_vec\n",
    "    vec\n",
    "}\n",
    "\n",
    "X<-data.frame(apply(features_set[, numeric_variables_idx], MARGIN=2, replace_na_with_mean_value))\n",
    "miss_idx = which(is.na(features_set[,numeric_variables_idx]), arr.ind = T)\n",
    "non_miss_idx <- which(!is.na(features_set[,numeric_variables_idx]), arr.ind = T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in rankMatrix(X):\n",
      "“rankMatrix(<large sparse Matrix>, method = 'tolNorm2') coerces to dense matrix.\n",
      " Probably should rather use method = 'qr' !?”\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>3</li><li>53415</li><li>26</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3\n",
       "\\item 53415\n",
       "\\item 26\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3\n",
       "2. 53415\n",
       "3. 26\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]     3 53415    26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] : NMF with k= 24\"\n",
      "[1] \"[INFO] : iter 0  Relative error is : 102.379350736508\"\n",
      "[1] \"[INFO] : iter 100  Relative error is : 0.049836856952929\"\n",
      "[1] \"[INFO] : iter 200  Relative error is : 0.037363236108705\"\n",
      "[1] \"[INFO] : iter 300  Relative error is : 0.032979480310818\"\n",
      "[1] \"[INFO] : iter 400  Relative error is : 0.0308040094724638\"\n",
      "[1] \"[INFO] : iter 500  Relative error is : 0.0284114313381648\"\n",
      "[1] \"[INFO] : iter 600  Relative error is : 0.0267378554082752\"\n",
      "[1] \"[INFO] : iter 700  Relative error is : 0.0250835127026672\"\n",
      "[1] \"Computed nmf, final dist is 0.0155656872304426\"\n",
      "[1] \"[INFO] : NMF with k= 25\"\n",
      "[1] \"[INFO] : iter 0  Relative error is : 105.88639192926\"\n",
      "[1] \"[INFO] : iter 100  Relative error is : 0.0376683274772725\"\n",
      "[1] \"[INFO] : iter 200  Relative error is : 0.0290107127327798\"\n",
      "[1] \"[INFO] : iter 300  Relative error is : 0.0236154637767103\"\n",
      "[1] \"[INFO] : iter 400  Relative error is : 0.0214581011709531\"\n",
      "[1] \"[INFO] : iter 500  Relative error is : 0.020394299311901\"\n",
      "[1] \"[INFO] : iter 600  Relative error is : 0.0196941366543559\"\n",
      "[1] \"[INFO] : iter 700  Relative error is : 0.0193099283081424\"\n",
      "[1] \"Computed nmf, final dist is 0.0103518832524261\"\n",
      "[1] \"[INFO] : NMF with k= 26\"\n",
      "[1] \"[INFO] : iter 0  Relative error is : 109.076227768822\"\n",
      "[1] \"[INFO] : iter 100  Relative error is : 0.0401347340890354\"\n",
      "[1] \"[INFO] : iter 200  Relative error is : 0.0295235330692998\"\n",
      "[1] \"[INFO] : iter 300  Relative error is : 0.026468830521451\"\n",
      "[1] \"[INFO] : iter 400  Relative error is : 0.0239561188959172\"\n",
      "[1] \"[INFO] : iter 500  Relative error is : 0.0219725898891598\"\n",
      "[1] \"[INFO] : iter 600  Relative error is : 0.0199970261282536\"\n",
      "[1] \"[INFO] : iter 700  Relative error is : 0.0191195722570502\"\n",
      "[1] \"Computed nmf, final dist is 0.00943636657318307\"\n"
     ]
    }
   ],
   "source": [
    "# 2: nNMF for k=k1 to K=kN with masking missing values\n",
    "library(Matrix) # For rankMatrix \n",
    "rX <- rankMatrix(X)\n",
    "k1 = floor(max(abs(rX - N/2), 1))\n",
    "kN = min(abs(rX + N/2), dim(X)[1], dim(X)[2])\n",
    "\n",
    "X_hat = array(0, dim = c(kN-k1+1, nrow(X), ncol(X)))\n",
    "dim(X_hat)\n",
    "for (K in k1:kN) {\n",
    "    # compute NMF\n",
    "    nmf <- nfm_mult_upd(X, K, missing_idx = miss_idx, maxit=800)\n",
    "    print(paste(\"Computed nmf, final dist is\", nmf$dst))\n",
    "    X_hat[K-k1+1,,] <- nmf$res\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: weighted reconstruction\n",
    "d = array(0, dim = c(kN-k1+1))\n",
    "for (K in 1:(kN-k1+1)) {\n",
    "    # Reconstruction error based on non missing values\n",
    "    d[K] <- sum(abs(X_hat[K,,][non_miss_idx] - X[non_miss_idx]))/nrow(non_miss_idx)\n",
    "}\n",
    "\n",
    "X_hat_f <- matrix(0, nrow = nrow(X), ncol = ncol(X))\n",
    "denum <- 0\n",
    "for (K in 1:(kN-k1+1)) {\n",
    "    # Reconstruction matrix\n",
    "    X_hat_f <- X_hat_f + exp(-d[K])*X_hat[K,,]\n",
    "    denum <- denum + exp(-d[K])\n",
    "}\n",
    "X_hat_f <- X_hat_f / sum(exp(-d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       V1                  V2                  V3          \n",
       " Min.   :0.0000314   Min.   :0.0000036   Min.   :0.000000  \n",
       " 1st Qu.:0.3456898   1st Qu.:0.4999746   1st Qu.:0.004629  \n",
       " Median :0.5773749   Median :0.5000038   Median :0.018822  \n",
       " Mean   :0.5435452   Mean   :0.6309477   Mean   :0.051731  \n",
       " 3rd Qu.:0.6830849   3rd Qu.:0.9999794   3rd Qu.:0.034775  \n",
       " Max.   :1.1058737   Max.   :1.0004168   Max.   :0.666693  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  h1n1_concern    h1n1_knowledge   behavioral_antiviral_meds\n",
       " Min.   :0.0000   Min.   :0.0000   Min.   :0.00000          \n",
       " 1st Qu.:0.3333   1st Qu.:0.5000   1st Qu.:0.00000          \n",
       " Median :0.6667   Median :0.5000   Median :0.00000          \n",
       " Mean   :0.5391   Mean   :0.6294   Mean   :0.04925          \n",
       " 3rd Qu.:0.6667   3rd Qu.:1.0000   3rd Qu.:0.00000          \n",
       " Max.   :1.0000   Max.   :1.0000   Max.   :1.00000          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(X_hat_f[, c(1, 2, 3)])\n",
    "summary(X[, c(1, 2, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have similar summaries but if we looked at the variances the NNMF would give more realistic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace in the feature set\n",
    "X[miss_idx] <- X_hat_f[miss_idx]\n",
    "features_set[, numeric_variables_idx] <- X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we managed numerical variabels we can work on encoding the categorical ones which we did not work on yet.\n",
    "\n",
    "Getting back to the columns containing many missing values, we use one hot encoding and create a category \"missing\", considering that not having this piece of information could be meaning something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_nas <- which(sapply(test_set_features, function(x) sum(is.na(x))) > 10000)\n",
    "\n",
    "library(fastDummies)\n",
    "features_set <- dummy_cols(features_set, \n",
    "                           select_columns = names(useful_nas),\n",
    "                           remove_selected_columns = T)\n",
    "\n",
    "## Change NAs to 0 for the new one hot encoded columns\n",
    "replace_na_with_0<-function(vec) {\n",
    "    vec[is.na(vec)]<-0\n",
    "    vec\n",
    "}\n",
    "\n",
    "useful_nas_feat_idx <- c(grep(\"health_insurance_*\", colnames(features_set)))\n",
    "useful_nas_feat_idx <- c(useful_nas_feat_idx, grep(\"employment_industry*\", colnames(features_set)))\n",
    "useful_nas_feat_idx <- c(useful_nas_feat_idx, grep(\"employment_occup*\", colnames(features_set)))\n",
    "\n",
    "features_set[,useful_nas_feat_idx] <- replace_na_with_0(features_set[,useful_nas_feat_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining columns will be treated the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " respondent_id    h1n1_concern    h1n1_knowledge   behavioral_antiviral_meds\n",
       " Min.   :    0   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000          \n",
       " 1st Qu.:13354   1st Qu.:0.3333   1st Qu.:0.5000   1st Qu.:0.00000          \n",
       " Median :26707   Median :0.6667   Median :0.5000   Median :0.00000          \n",
       " Mean   :26707   Mean   :0.5391   Mean   :0.6294   Mean   :0.04925          \n",
       " 3rd Qu.:40060   3rd Qu.:0.6667   3rd Qu.:1.0000   3rd Qu.:0.00000          \n",
       " Max.   :53414   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000          \n",
       " behavioral_avoidance behavioral_face_mask behavioral_wash_hands\n",
       " Min.   :0.0000       Min.   :0.00000      Min.   :0.0000       \n",
       " 1st Qu.:0.0000       1st Qu.:0.00000      1st Qu.:1.0000       \n",
       " Median :1.0000       Median :0.00000      Median :1.0000       \n",
       " Mean   :0.7222       Mean   :0.06909      Mean   :0.8246       \n",
       " 3rd Qu.:1.0000       3rd Qu.:0.00000      3rd Qu.:1.0000       \n",
       " Max.   :1.0000       Max.   :1.00000      Max.   :1.0000       \n",
       " behavioral_large_gatherings behavioral_outside_home behavioral_touch_face\n",
       " Min.   :0.000               Min.   :0.0000          Min.   :0.0000       \n",
       " 1st Qu.:0.000               1st Qu.:0.0000          1st Qu.:0.0000       \n",
       " Median :0.000               Median :0.0000          Median :1.0000       \n",
       " Mean   :0.354               Mean   :0.3362          Mean   :0.6774       \n",
       " 3rd Qu.:1.000               3rd Qu.:1.0000          3rd Qu.:1.0000       \n",
       " Max.   :1.000               Max.   :1.0000          Max.   :1.0000       \n",
       " doctor_recc_h1n1    doctor_recc_seasonal chronic_med_condition\n",
       " Min.   :0.0000000   Min.   :0.0000       Min.   :0.0000       \n",
       " 1st Qu.:0.0000000   1st Qu.:0.0000       1st Qu.:0.0000       \n",
       " Median :0.0000000   Median :0.0000       Median :0.0000       \n",
       " Mean   :0.2036703   Mean   :0.3049       Mean   :0.2721       \n",
       " 3rd Qu.:0.0000873   3rd Qu.:1.0000       3rd Qu.:1.0000       \n",
       " Max.   :1.0000000   Max.   :1.0000       Max.   :1.0000       \n",
       " child_under_6_months health_worker    opinion_h1n1_vacc_effective\n",
       " Min.   :0.00000      Min.   :0.0000   Min.   :0.0000             \n",
       " 1st Qu.:0.00000      1st Qu.:0.0000   1st Qu.:0.5000             \n",
       " Median :0.00000      Median :0.0000   Median :0.7500             \n",
       " Mean   :0.08246      Mean   :0.1084   Mean   :0.7029             \n",
       " 3rd Qu.:0.00000      3rd Qu.:0.0000   3rd Qu.:1.0000             \n",
       " Max.   :1.00000      Max.   :1.0000   Max.   :1.0000             \n",
       " opinion_h1n1_risk opinion_h1n1_sick_from_vacc opinion_seas_vacc_effective\n",
       " Min.   :0.000     Min.   :0.0000              Min.   :0.0000             \n",
       " 1st Qu.:0.000     1st Qu.:0.0000              1st Qu.:0.7500             \n",
       " Median :0.250     Median :0.2500              Median :0.7500             \n",
       " Mean   :0.329     Mean   :0.3354              Mean   :0.7454             \n",
       " 3rd Qu.:0.750     3rd Qu.:0.7500              3rd Qu.:1.0000             \n",
       " Max.   :1.000     Max.   :1.0000              Max.   :1.0000             \n",
       " opinion_seas_risk opinion_seas_sick_from_vacc   age_group     \n",
       " Min.   :0.0000    Min.   :0.0000              Min.   :0.0000  \n",
       " 1st Qu.:0.2500    1st Qu.:0.0000              1st Qu.:0.2500  \n",
       " Median :0.2500    Median :0.2500              Median :0.5000  \n",
       " Mean   :0.4206    Mean   :0.2772              Mean   :0.5445  \n",
       " 3rd Qu.:0.7500    3rd Qu.:0.5000              3rd Qu.:1.0000  \n",
       " Max.   :1.0000    Max.   :1.0000              Max.   :1.0000  \n",
       "   education      income_poverty        census_msa    household_adults\n",
       " Min.   :0.0000   Min.   :0.0000000   Min.   :0.000   Min.   :0.0000  \n",
       " 1st Qu.:0.3333   1st Qu.:0.0000000   1st Qu.:0.000   1st Qu.:0.0000  \n",
       " Median :0.6667   Median :0.0001434   Median :0.500   Median :0.3333  \n",
       " Mean   :0.5899   Mean   :0.2267656   Mean   :0.412   Mean   :0.2944  \n",
       " 3rd Qu.:1.0000   3rd Qu.:0.5000000   3rd Qu.:1.000   3rd Qu.:0.3333  \n",
       " Max.   :1.0000   Max.   :1.0000000   Max.   :1.000   Max.   :1.0000  \n",
       " household_children health_insurance_0 health_insurance_1 health_insurance_NA\n",
       " Min.   :0.0000     Min.   :0.00000    Min.   :0.0000     Min.   :0.0000     \n",
       " 1st Qu.:0.0000     1st Qu.:0.00000    1st Qu.:0.0000     1st Qu.:0.0000     \n",
       " Median :0.0000     Median :0.00000    Median :0.0000     Median :0.0000     \n",
       " Mean   :0.1782     Mean   :0.06288    Mean   :0.4784     Mean   :0.4587     \n",
       " 3rd Qu.:0.3333     3rd Qu.:0.00000    3rd Qu.:1.0000     3rd Qu.:1.0000     \n",
       " Max.   :1.0000     Max.   :1.00000    Max.   :1.0000     Max.   :1.0000     \n",
       " employment_industry_arjwrbjb employment_industry_atmlpfrs\n",
       " Min.   :0.00000              Min.   :0.00000             \n",
       " 1st Qu.:0.00000              1st Qu.:0.00000             \n",
       " Median :0.00000              Median :0.00000             \n",
       " Mean   :0.03287              Mean   :0.03272             \n",
       " 3rd Qu.:0.00000              3rd Qu.:0.00000             \n",
       " Max.   :1.00000              Max.   :1.00000             \n",
       " employment_industry_cfqqtusy employment_industry_dotnnunm\n",
       " Min.   :0.00000              Min.   :0.000000            \n",
       " 1st Qu.:0.00000              1st Qu.:0.000000            \n",
       " Median :0.00000              Median :0.000000            \n",
       " Mean   :0.01178              Mean   :0.007882            \n",
       " 3rd Qu.:0.00000              3rd Qu.:0.000000            \n",
       " Max.   :1.00000              Max.   :1.000000            \n",
       " employment_industry_fcxhlnwr employment_industry_haxffmxo\n",
       " Min.   :0.00000              Min.   :0.000000            \n",
       " 1st Qu.:0.00000              1st Qu.:0.000000            \n",
       " Median :0.00000              Median :0.000000            \n",
       " Mean   :0.09271              Mean   :0.005766            \n",
       " 3rd Qu.:0.00000              3rd Qu.:0.000000            \n",
       " Max.   :1.00000              Max.   :1.000000            \n",
       " employment_industry_ldnlellj employment_industry_mcubkhph\n",
       " Min.   :0.00000              Min.   :0.00000             \n",
       " 1st Qu.:0.00000              1st Qu.:0.00000             \n",
       " Median :0.00000              Median :0.00000             \n",
       " Mean   :0.04596              Mean   :0.01005             \n",
       " 3rd Qu.:0.00000              3rd Qu.:0.00000             \n",
       " Max.   :1.00000              Max.   :1.00000             \n",
       " employment_industry_mfikgejo employment_industry_msuufmds\n",
       " Min.   :0.00000              Min.   :0.000000            \n",
       " 1st Qu.:0.00000              1st Qu.:0.000000            \n",
       " Median :0.00000              Median :0.000000            \n",
       " Mean   :0.02364              Mean   :0.004793            \n",
       " 3rd Qu.:0.00000              3rd Qu.:0.000000            \n",
       " Max.   :1.00000              Max.   :1.000000            \n",
       " employment_industry_nduyfdeo employment_industry_phxvnwax\n",
       " Min.   :0.00000              Min.   :0.000000            \n",
       " 1st Qu.:0.00000              1st Qu.:0.000000            \n",
       " Median :0.00000              Median :0.000000            \n",
       " Mean   :0.01082              Mean   :0.003183            \n",
       " 3rd Qu.:0.00000              3rd Qu.:0.000000            \n",
       " Max.   :1.00000              Max.   :1.000000            \n",
       " employment_industry_pxcmvdjn employment_industry_qnlwzans\n",
       " Min.   :0.0000               Min.   :0.0000000           \n",
       " 1st Qu.:0.0000               1st Qu.:0.0000000           \n",
       " Median :0.0000               Median :0.0000000           \n",
       " Mean   :0.0396               Mean   :0.0003557           \n",
       " 3rd Qu.:0.0000               3rd Qu.:0.0000000           \n",
       " Max.   :1.0000               Max.   :1.0000000           \n",
       " employment_industry_rucpziij employment_industry_saaquncn\n",
       " Min.   :0.00000              Min.   :0.00000             \n",
       " 1st Qu.:0.00000              1st Qu.:0.00000             \n",
       " Median :0.00000              Median :0.00000             \n",
       " Mean   :0.01986              Mean   :0.01266             \n",
       " 3rd Qu.:0.00000              3rd Qu.:0.00000             \n",
       " Max.   :1.00000              Max.   :1.00000             \n",
       " employment_industry_vjjrobsf employment_industry_wlfvacwt\n",
       " Min.   :0.00000              Min.   :0.000000            \n",
       " 1st Qu.:0.00000              1st Qu.:0.000000            \n",
       " Median :0.00000              Median :0.000000            \n",
       " Mean   :0.02048              Mean   :0.007994            \n",
       " 3rd Qu.:0.00000              3rd Qu.:0.000000            \n",
       " Max.   :1.00000              Max.   :1.000000            \n",
       " employment_industry_wxleyezf employment_industry_xicduogh\n",
       " Min.   :0.00000              Min.   :0.00000             \n",
       " 1st Qu.:0.00000              1st Qu.:0.00000             \n",
       " Median :0.00000              Median :0.00000             \n",
       " Mean   :0.06757              Mean   :0.03179             \n",
       " 3rd Qu.:0.00000              3rd Qu.:0.00000             \n",
       " Max.   :1.00000              Max.   :1.00000             \n",
       " employment_industry_xqicxuve employment_industry_NA\n",
       " Min.   :0.00000              Min.   :0.0000        \n",
       " 1st Qu.:0.00000              1st Qu.:0.0000        \n",
       " Median :0.00000              Median :0.0000        \n",
       " Mean   :0.01943              Mean   :0.4981        \n",
       " 3rd Qu.:0.00000              3rd Qu.:1.0000        \n",
       " Max.   :1.00000              Max.   :1.0000        \n",
       " employment_occupation_bxpfxfdn employment_occupation_ccgxvspp\n",
       " Min.   :0.00000                Min.   :0.00000               \n",
       " 1st Qu.:0.00000                1st Qu.:0.00000               \n",
       " Median :0.00000                Median :0.00000               \n",
       " Mean   :0.01239                Mean   :0.01327               \n",
       " 3rd Qu.:0.00000                3rd Qu.:0.00000               \n",
       " Max.   :1.00000                Max.   :1.00000               \n",
       " employment_occupation_cmhcxjea employment_occupation_dcjcmpih\n",
       " Min.   :0.00000                Min.   :0.000000              \n",
       " 1st Qu.:0.00000                1st Qu.:0.000000              \n",
       " Median :0.00000                Median :0.000000              \n",
       " Mean   :0.04626                Mean   :0.005785              \n",
       " 3rd Qu.:0.00000                3rd Qu.:0.000000              \n",
       " Max.   :1.00000                Max.   :1.000000              \n",
       " employment_occupation_dlvbwzss employment_occupation_emcorrxb\n",
       " Min.   :0.000000               Min.   :0.00000               \n",
       " 1st Qu.:0.000000               1st Qu.:0.00000               \n",
       " Median :0.000000               Median :0.00000               \n",
       " Mean   :0.008837               Mean   :0.04781               \n",
       " 3rd Qu.:0.000000               3rd Qu.:0.00000               \n",
       " Max.   :1.000000               Max.   :1.00000               \n",
       " employment_occupation_haliazsg employment_occupation_hfxkjkmi\n",
       " Min.   :0.00000                Min.   :0.00000               \n",
       " 1st Qu.:0.00000                1st Qu.:0.00000               \n",
       " Median :0.00000                Median :0.00000               \n",
       " Mean   :0.01116                Mean   :0.02791               \n",
       " 3rd Qu.:0.00000                3rd Qu.:0.00000               \n",
       " Max.   :1.00000                Max.   :1.00000               \n",
       " employment_occupation_hodpvpew employment_occupation_kldqjyjy\n",
       " Min.   :0.00000                Min.   :0.00000               \n",
       " 1st Qu.:0.00000                1st Qu.:0.00000               \n",
       " Median :0.00000                Median :0.00000               \n",
       " Mean   :0.00863                Mean   :0.01713               \n",
       " 3rd Qu.:0.00000                3rd Qu.:0.00000               \n",
       " Max.   :1.00000                Max.   :1.00000               \n",
       " employment_occupation_mxkfnird employment_occupation_oijqvulv\n",
       " Min.   :0.00000                Min.   :0.00000               \n",
       " 1st Qu.:0.00000                1st Qu.:0.00000               \n",
       " Median :0.00000                Median :0.00000               \n",
       " Mean   :0.05747                Mean   :0.01292               \n",
       " 3rd Qu.:0.00000                3rd Qu.:0.00000               \n",
       " Max.   :1.00000                Max.   :1.00000               \n",
       " employment_occupation_pvmttkik employment_occupation_qxajmpny\n",
       " Min.   :0.000000               Min.   :0.0000                \n",
       " 1st Qu.:0.000000               1st Qu.:0.0000                \n",
       " Median :0.000000               Median :0.0000                \n",
       " Mean   :0.003857               Mean   :0.0196                \n",
       " 3rd Qu.:0.000000               3rd Qu.:0.0000                \n",
       " Max.   :1.000000               Max.   :1.0000                \n",
       " employment_occupation_rcertsgn employment_occupation_tfqavkke\n",
       " Min.   :0.00000                Min.   :0.00000               \n",
       " 1st Qu.:0.00000                1st Qu.:0.00000               \n",
       " Median :0.00000                Median :0.00000               \n",
       " Mean   :0.01073                Mean   :0.01479               \n",
       " 3rd Qu.:0.00000                3rd Qu.:0.00000               \n",
       " Max.   :1.00000                Max.   :1.00000               \n",
       " employment_occupation_ukymxvdu employment_occupation_uqqtjvyb\n",
       " Min.   :0.00000                Min.   :0.00000               \n",
       " 1st Qu.:0.00000                1st Qu.:0.00000               \n",
       " Median :0.00000                Median :0.00000               \n",
       " Mean   :0.01312                Mean   :0.01784               \n",
       " 3rd Qu.:0.00000                3rd Qu.:0.00000               \n",
       " Max.   :1.00000                Max.   :1.00000               \n",
       " employment_occupation_vlluhbov employment_occupation_xgwztkwe\n",
       " Min.   :0.00000                Min.   :0.00000               \n",
       " 1st Qu.:0.00000                1st Qu.:0.00000               \n",
       " Median :0.00000                Median :0.00000               \n",
       " Mean   :0.01271                Mean   :0.04156               \n",
       " 3rd Qu.:0.00000                3rd Qu.:0.00000               \n",
       " Max.   :1.00000                Max.   :1.00000               \n",
       " employment_occupation_xqwwgdyp employment_occupation_xtkaffoo\n",
       " Min.   :0.00000                Min.   :0.00000               \n",
       " 1st Qu.:0.00000                1st Qu.:0.00000               \n",
       " Median :0.00000                Median :0.00000               \n",
       " Mean   :0.01797                Mean   :0.06601               \n",
       " 3rd Qu.:0.00000                3rd Qu.:0.00000               \n",
       " Max.   :1.00000                Max.   :1.00000               \n",
       " employment_occupation_xzmlyyjv employment_occupation_NA   race_Black     \n",
       " Min.   :0.000000               Min.   :0.0000           Min.   :0.00000  \n",
       " 1st Qu.:0.000000               1st Qu.:0.0000           1st Qu.:0.00000  \n",
       " Median :0.000000               Median :1.0000           Median :0.00000  \n",
       " Mean   :0.008687               Mean   :0.5035           Mean   :0.08035  \n",
       " 3rd Qu.:0.000000               3rd Qu.:1.0000           3rd Qu.:0.00000  \n",
       " Max.   :1.000000               Max.   :1.0000           Max.   :1.00000  \n",
       " race_Hispanic     race_Other or Multiple   race_White       sex_Female    \n",
       " Min.   :0.00000   Min.   :0.00000        Min.   :0.0000   Min.   :0.0000  \n",
       " 1st Qu.:0.00000   1st Qu.:0.00000        1st Qu.:1.0000   1st Qu.:0.0000  \n",
       " Median :0.00000   Median :0.00000        Median :1.0000   Median :1.0000  \n",
       " Mean   :0.06474   Mean   :0.05882        Mean   :0.7961   Mean   :0.5933  \n",
       " 3rd Qu.:0.00000   3rd Qu.:0.00000        3rd Qu.:1.0000   3rd Qu.:1.0000  \n",
       " Max.   :1.00000   Max.   :1.00000        Max.   :1.0000   Max.   :1.0000  \n",
       "    sex_Male      marital_status_Married marital_status_Not Married\n",
       " Min.   :0.0000   Min.   :0.0000         Min.   :0.0000            \n",
       " 1st Qu.:0.0000   1st Qu.:0.0000         1st Qu.:0.0000            \n",
       " Median :0.0000   Median :1.0000         Median :0.0000            \n",
       " Mean   :0.4067   Mean   :0.5074         Mean   :0.4392            \n",
       " 3rd Qu.:1.0000   3rd Qu.:1.0000         3rd Qu.:1.0000            \n",
       " Max.   :1.0000   Max.   :1.0000         Max.   :1.0000            \n",
       " marital_status_NA rent_or_own_Own  rent_or_own_Rent rent_or_own_NA   \n",
       " Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n",
       " 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  \n",
       " Median :0.00000   Median :1.0000   Median :0.0000   Median :0.00000  \n",
       " Mean   :0.05336   Mean   :0.7012   Mean   :0.2224   Mean   :0.07635  \n",
       " 3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n",
       " Max.   :1.00000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n",
       " employment_status_Employed employment_status_Not in Labor Force\n",
       " Min.   :0.0000             Min.   :0.0000                      \n",
       " 1st Qu.:0.0000             1st Qu.:0.0000                      \n",
       " Median :1.0000             Median :0.0000                      \n",
       " Mean   :0.5091             Mean   :0.3805                      \n",
       " 3rd Qu.:1.0000             3rd Qu.:1.0000                      \n",
       " Max.   :1.0000             Max.   :1.0000                      \n",
       " employment_status_Unemployed employment_status_NA hhs_geo_region_atmpeygn\n",
       " Min.   :0.00000              Min.   :0.00000      Min.   :0.00000        \n",
       " 1st Qu.:0.00000              1st Qu.:0.00000      1st Qu.:0.00000        \n",
       " Median :0.00000              Median :0.00000      Median :0.00000        \n",
       " Mean   :0.05553              Mean   :0.05493      Mean   :0.07524        \n",
       " 3rd Qu.:0.00000              3rd Qu.:0.00000      3rd Qu.:0.00000        \n",
       " Max.   :1.00000              Max.   :1.00000      Max.   :1.00000        \n",
       " hhs_geo_region_bhuqouqj hhs_geo_region_dqpwygqj hhs_geo_region_fpwskwrf\n",
       " Min.   :0.000           Min.   :0.00000         Min.   :0.0000         \n",
       " 1st Qu.:0.000           1st Qu.:0.00000         1st Qu.:0.0000         \n",
       " Median :0.000           Median :0.00000         Median :0.0000         \n",
       " Mean   :0.107           Mean   :0.04263         Mean   :0.1218         \n",
       " 3rd Qu.:0.000           3rd Qu.:0.00000         3rd Qu.:0.0000         \n",
       " Max.   :1.000           Max.   :1.00000         Max.   :1.0000         \n",
       " hhs_geo_region_kbazzjca hhs_geo_region_lrircsnp hhs_geo_region_lzgpxyit\n",
       " Min.   :0.0000          Min.   :0.00000         Min.   :0.0000         \n",
       " 1st Qu.:0.0000          1st Qu.:0.00000         1st Qu.:0.0000         \n",
       " Median :0.0000          Median :0.00000         Median :0.0000         \n",
       " Mean   :0.1067          Mean   :0.07676         Mean   :0.1621         \n",
       " 3rd Qu.:0.0000          3rd Qu.:0.00000         3rd Qu.:0.0000         \n",
       " Max.   :1.0000          Max.   :1.00000         Max.   :1.0000         \n",
       " hhs_geo_region_mlyzmhmf hhs_geo_region_oxchjgsf hhs_geo_region_qufhixun\n",
       " Min.   :0.00000         Min.   :0.0000          Min.   :0.0000         \n",
       " 1st Qu.:0.00000         1st Qu.:0.0000          1st Qu.:0.0000         \n",
       " Median :0.00000         Median :0.0000          Median :0.0000         \n",
       " Mean   :0.08286         Mean   :0.1094          Mean   :0.1154         \n",
       " 3rd Qu.:0.00000         3rd Qu.:0.0000          3rd Qu.:0.0000         \n",
       " Max.   :1.00000         Max.   :1.0000          Max.   :1.0000         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "factor_variables<-which(sapply(features_set[1,],class)==\"factor\")\n",
    "factor_col_names <- colnames(factor_variables)\n",
    "\n",
    "features_set_int_encoded <- dummy_cols(features_set,\n",
    "                                       select_columns = factor_col_names,\n",
    "                                       ignore_na = F,\n",
    "                                       remove_selected_columns = T)\n",
    "\n",
    "\n",
    "features_set_int_encoded <- replace_na_with_0(features_set_int_encoded)\n",
    "\n",
    "tr_set_enc <- features_set_int_encoded[tr_indexes,]\n",
    "ts_set_enc <- features_set_int_encoded[ts_indexes,]\n",
    "\n",
    "summary(features_set_int_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA was used to select the best features. This technique creates new fewer but more representative features from the current set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Importance of components:\n",
       "                          PC1     PC2     PC3     PC4     PC5    PC6     PC7\n",
       "Standard deviation     1.0217 0.78070 0.75060 0.71353 0.66244 0.5738 0.55043\n",
       "Proportion of Variance 0.1217 0.07109 0.06571 0.05938 0.05118 0.0384 0.03534\n",
       "Cumulative Proportion  0.1217 0.19283 0.25854 0.31793 0.36911 0.4075 0.44285\n",
       "                           PC8     PC9    PC10    PC11    PC12   PC13    PC14\n",
       "Standard deviation     0.48662 0.46437 0.45430 0.42466 0.42035 0.4151 0.40546\n",
       "Proportion of Variance 0.02762 0.02515 0.02407 0.02103 0.02061 0.0201 0.01917\n",
       "Cumulative Proportion  0.47046 0.49562 0.51969 0.54072 0.56133 0.5814 0.60060\n",
       "                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\n",
       "Standard deviation     0.38819 0.37757 0.36287 0.34811 0.34411 0.33599 0.33011\n",
       "Proportion of Variance 0.01758 0.01663 0.01536 0.01413 0.01381 0.01317 0.01271\n",
       "Cumulative Proportion  0.61818 0.63481 0.65016 0.66430 0.67811 0.69128 0.70399\n",
       "                          PC22    PC23    PC24    PC25    PC26    PC27    PC28\n",
       "Standard deviation     0.32044 0.31776 0.31594 0.30661 0.30340 0.30070 0.29911\n",
       "Proportion of Variance 0.01198 0.01178 0.01164 0.01097 0.01074 0.01055 0.01043\n",
       "Cumulative Proportion  0.71596 0.72774 0.73938 0.75035 0.76108 0.77163 0.78206\n",
       "                          PC29    PC30    PC31    PC32    PC33    PC34    PC35\n",
       "Standard deviation     0.29247 0.27951 0.27683 0.27047 0.26949 0.26697 0.26476\n",
       "Proportion of Variance 0.00998 0.00911 0.00894 0.00853 0.00847 0.00831 0.00818\n",
       "Cumulative Proportion  0.79204 0.80115 0.81009 0.81862 0.82710 0.83541 0.84358\n",
       "                          PC36    PC37    PC38    PC39    PC40   PC41    PC42\n",
       "Standard deviation     0.25583 0.25403 0.24771 0.24522 0.24209 0.2414 0.23846\n",
       "Proportion of Variance 0.00763 0.00753 0.00716 0.00701 0.00684 0.0068 0.00663\n",
       "Cumulative Proportion  0.85122 0.85874 0.86590 0.87291 0.87975 0.8865 0.89318\n",
       "                          PC43   PC44   PC45    PC46    PC47    PC48    PC49\n",
       "Standard deviation     0.23122 0.2211 0.2112 0.20973 0.20628 0.20528 0.20400\n",
       "Proportion of Variance 0.00624 0.0057 0.0052 0.00513 0.00496 0.00492 0.00485\n",
       "Cumulative Proportion  0.89941 0.9051 0.9103 0.91545 0.92041 0.92533 0.93018\n",
       "                         PC50    PC51    PC52    PC53    PC54    PC55    PC56\n",
       "Standard deviation     0.2007 0.18885 0.18122 0.17655 0.17245 0.16784 0.16538\n",
       "Proportion of Variance 0.0047 0.00416 0.00383 0.00364 0.00347 0.00329 0.00319\n",
       "Cumulative Proportion  0.9349 0.93904 0.94287 0.94650 0.94997 0.95326 0.95645\n",
       "                          PC57    PC58    PC59    PC60    PC61    PC62    PC63\n",
       "Standard deviation     0.16111 0.15794 0.15274 0.14407 0.13438 0.13009 0.12878\n",
       "Proportion of Variance 0.00303 0.00291 0.00272 0.00242 0.00211 0.00197 0.00193\n",
       "Cumulative Proportion  0.95947 0.96238 0.96510 0.96752 0.96963 0.97160 0.97354\n",
       "                          PC64    PC65    PC66   PC67    PC68    PC69    PC70\n",
       "Standard deviation     0.12145 0.11622 0.11510 0.1133 0.11167 0.10733 0.10525\n",
       "Proportion of Variance 0.00172 0.00158 0.00155 0.0015 0.00145 0.00134 0.00129\n",
       "Cumulative Proportion  0.97526 0.97683 0.97838 0.9799 0.98133 0.98267 0.98397\n",
       "                          PC71   PC72    PC73    PC74    PC75    PC76    PC77\n",
       "Standard deviation     0.10348 0.1013 0.09967 0.09589 0.09557 0.09321 0.09215\n",
       "Proportion of Variance 0.00125 0.0012 0.00116 0.00107 0.00107 0.00101 0.00099\n",
       "Cumulative Proportion  0.98522 0.9864 0.98757 0.98864 0.98971 0.99072 0.99171\n",
       "                          PC78    PC79    PC80    PC81    PC82    PC83    PC84\n",
       "Standard deviation     0.08926 0.08813 0.08791 0.08522 0.08355 0.08302 0.08160\n",
       "Proportion of Variance 0.00093 0.00091 0.00090 0.00085 0.00081 0.00080 0.00078\n",
       "Cumulative Proportion  0.99264 0.99355 0.99445 0.99530 0.99611 0.99691 0.99769\n",
       "                          PC85    PC86    PC87    PC88    PC89    PC90     PC91\n",
       "Standard deviation     0.07245 0.06855 0.06389 0.05504 0.04854 0.01928 0.003124\n",
       "Proportion of Variance 0.00061 0.00055 0.00048 0.00035 0.00027 0.00004 0.000000\n",
       "Cumulative Proportion  0.99830 0.99885 0.99933 0.99968 0.99996 1.00000 1.000000\n",
       "                            PC92      PC93      PC94     PC95      PC96\n",
       "Standard deviation     2.158e-14 8.894e-15 7.808e-15 6.59e-15 3.653e-15\n",
       "Proportion of Variance 0.000e+00 0.000e+00 0.000e+00 0.00e+00 0.000e+00\n",
       "Cumulative Proportion  1.000e+00 1.000e+00 1.000e+00 1.00e+00 1.000e+00\n",
       "                            PC97      PC98      PC99     PC100\n",
       "Standard deviation     1.645e-15 4.792e-16 1.368e-16 9.199e-17\n",
       "Proportion of Variance 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
       "Cumulative Proportion  1.000e+00 1.000e+00 1.000e+00 1.000e+00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_features <- tr_set_enc[, -c(1)]\n",
    "training_labels <- training_set_labels\n",
    "label_1 <- training_labels[,\"h1n1_vaccine\"]\n",
    "label_2 <- training_labels[, \"seasonal_vaccine\"]\n",
    "labels <- cbind(h1n1_vaccine=label_1, seasonal_vaccine=label_2)\n",
    "\n",
    "testing_features <- ts_set_enc[, -c(1)]\n",
    "feature_set <- rbind(training_features, testing_features)\n",
    "feature_set <- feature_set[, -c(1)] # remove resp id\n",
    "pca <- prcomp(feature_set)\n",
    "summary(pca)\n",
    "\n",
    "pca_feat_set <- pca$x[,1:54]\n",
    "\n",
    "library(\"caret\")\n",
    "\n",
    "ss <- preProcess(as.data.frame(pca_feat_set), method=c(\"range\"))\n",
    "pca_feat_set <- predict(ss, as.data.frame(pca_feat_set))\n",
    "\n",
    "pca_tr_set <- pca_feat_set[1:26707,]\n",
    "pca_ts_set <- pca_feat_set[26708:53415,]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the 54 features was made on the basis of the analysis of the pca summary. Indeed, the standard deviations and variances allowed us to conclude that 54 features were sufficient to train a model. The cumulative proportion also shows that after taking 54 features, 95% of the variance of the problem was defined, which seems enough considering the division by tzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first thing to define is the way in which the models will be compared and evaluated. To fit with the way the Driven Data plateforne is evaluating scores, the ROC and AUC are used to estimate the efficienty of a method. Those two mesures are computed with the followinf code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUCROC <- function(Y_pred, Y, title_of_curve) {\n",
    "    thresholds <- seq(0,0.99,0.05)\n",
    "    FPR <- c()\n",
    "    TPR <- c()\n",
    "\n",
    "    for(threshold in thresholds){\n",
    "      Y_hat <- ifelse(Y_pred > threshold,1,0) \n",
    "      confusion_matrix <- table(Y_hat,Y)\n",
    "\n",
    "      if(dim(confusion_matrix)[1] < 2){ \n",
    "        if(rownames(confusion_matrix) == 0){\n",
    "          confusion_matrix <- rbind(confusion_matrix,c(0,0))\n",
    "          rownames(confusion_matrix)[2] <- 1\n",
    "        }\n",
    "        if(rownames(confusion_matrix) == 1){\n",
    "          confusion_matrix <- rbind(c(0,0),confusion_matrix)\n",
    "          rownames(confusion_matrix)[1] <- 0\n",
    "        }\n",
    "      }\n",
    "\n",
    "      FP <- confusion_matrix[2,1]\n",
    "      TP <- confusion_matrix[2,2]\n",
    "      N_N <- sum(confusion_matrix[,1]) # Total number of 0's\n",
    "      N_P <- sum(confusion_matrix[,2]) # Total number of 1's\n",
    "\n",
    "      FPR <- c(FPR,FP/N_N)\n",
    "      TPR <- c(TPR,TP/N_P)\n",
    "    }\n",
    "\n",
    "    plot.new()\n",
    "    plot(FPR,TPR)\n",
    "    lines(FPR,TPR,col=\"blue\")\n",
    "    lines(thresholds,thresholds,lty=2)\n",
    "    title(title_of_curve)\n",
    "    AUC <- sum(abs(diff(FPR)) * (head(TPR,-1)+tail(TPR,-1)))/2\n",
    "    return (AUC)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "First the linear model is going to be used. It is one the fastest model to train and it will give a good idea of the kind of relation that exists between features and labels. The only difference with the model viewed in practicals is that two outputs are predicted at a time, it means that the train set needs to have the two labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_set <- cbind(pca_tr_set, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making the prediction for the given test values, it is important to check that the model fits by doing a cross-validation and calculating the AUC and ROC. First, the cross-validation with the multivariate linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation of the train in two groups in order to calculate the error between prediction and reality.\n",
    "vaccine_idx <- sample(1:nrow(total_train_set))\n",
    "half_split <- floor(nrow(total_train_set)/2)\n",
    "train_data_set <- total_train_set[vaccine_idx[1:half_split],]\n",
    "test_data <- total_train_set[vaccine_idx[(half_split+1):nrow(total_train_set)],]\n",
    "target_idx <- ncol(train_data_set)\n",
    "targets <- c(target_idx, target_idx-1)\n",
    "\n",
    "# Linear model & prediction\n",
    "model <- lm(cbind(h1n1_vaccine, seasonal_vaccine)~., data=train_data_set)\n",
    "Y_pred <- predict(model,test_data[,-targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] - Training set size: 12017 - Testing set size 1336\"\n",
      "[1] \"[INFO] - Misclassification rate h1n1 - 1 fold: 0.172155688622755\"\n",
      "[1] \"[INFO] - Misclassification rate seasonal - 1 fold: 0.226796407185629\"\n",
      "[1] \"[INFO] - Training set size: 12017 - Testing set size 1336\"\n",
      "[1] \"[INFO] - Misclassification rate h1n1 - 2 fold: 0.148203592814371\"\n",
      "[1] \"[INFO] - Misclassification rate seasonal - 2 fold: 0.235029940119761\"\n",
      "[1] \"[INFO] - Training set size: 12017 - Testing set size 1336\"\n",
      "[1] \"[INFO] - Misclassification rate h1n1 - 3 fold: 0.151197604790419\"\n",
      "[1] \"[INFO] - Misclassification rate seasonal - 3 fold: 0.238023952095808\"\n",
      "[1] \"[INFO] - Training set size: 12017 - Testing set size 1336\"\n",
      "[1] \"[INFO] - Misclassification rate h1n1 - 4 fold: 0.159431137724551\"\n",
      "[1] \"[INFO] - Misclassification rate seasonal - 4 fold: 0.237275449101796\"\n",
      "[1] \"[INFO] - Training set size: 12017 - Testing set size 1336\"\n",
      "[1] \"[INFO] - Misclassification rate h1n1 - 5 fold: 0.158682634730539\"\n",
      "[1] \"[INFO] - Misclassification rate seasonal - 5 fold: 0.217065868263473\"\n",
      "[1] \"[INFO] - Training set size: 12017 - Testing set size 1336\"\n",
      "[1] \"[INFO] - Misclassification rate h1n1 - 6 fold: 0.154191616766467\"\n",
      "[1] \"[INFO] - Misclassification rate seasonal - 6 fold: 0.24251497005988\"\n",
      "[1] \"[INFO] - Training set size: 12017 - Testing set size 1336\"\n",
      "[1] \"[INFO] - Misclassification rate h1n1 - 7 fold: 0.157185628742515\"\n",
      "[1] \"[INFO] - Misclassification rate seasonal - 7 fold: 0.191616766467066\"\n",
      "[1] \"[INFO] - Training set size: 12017 - Testing set size 1336\"\n",
      "[1] \"[INFO] - Misclassification rate h1n1 - 8 fold: 0.153443113772455\"\n",
      "[1] \"[INFO] - Misclassification rate seasonal - 8 fold: 0.229041916167665\"\n",
      "[1] \"[INFO] - Training set size: 12017 - Testing set size 1336\"\n",
      "[1] \"[INFO] - Misclassification rate h1n1 - 9 fold: 0.141467065868264\"\n",
      "[1] \"[INFO] - Misclassification rate seasonal - 9 fold: 0.233532934131736\"\n",
      "[1] \"[INFO] - Training set size: 12024 - Testing set size 1329\"\n",
      "[1] \"[INFO] - Misclassification rate h1n1 - 10 fold: 0.168547780285929\"\n",
      "[1] \"[INFO] - Misclassification rate seasonal - 10 fold: 0.240782543265613\"\n",
      "[1] \"[INFO] - Mean misclassification rate h1n1: 0.156450586411827\"\n",
      "[1] \"[INFO] - Mean misclassification rate seasonal: 0.229168074685843\"\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "accuracy_vec_h1n1 <- array(0,k)\n",
    "accuracy_vec_seasonal <- array(0,k)\n",
    "threshold <- 0.5\n",
    "\n",
    "# 1. Shuffle the dataset randomly.\n",
    "vaccine_idx <- sample(1:nrow(train_data_set))\n",
    "\n",
    "# 2. Split the dataset into k groups\n",
    "max <- ceiling(nrow(train_data_set)/k)\n",
    "splits <- split(vaccine_idx, ceiling(seq_along(vaccine_idx)/max))\n",
    "\n",
    "# 3. For each unique group:\n",
    "for (i in 1:k){\n",
    "  #3.1 Take the group as a hold out or test data set\n",
    "  test_data <- train_data_set[splits[[i]],]\n",
    "  \n",
    "  #3.2 Take the remaining groups as a training data set\n",
    "  train_data <- train_data_set[-splits[[i]],]\n",
    "  print(paste(\"[INFO] - Training set size:\",dim(train_data)[1],\"- Testing set size\",dim(test_data)[1]))\n",
    "  \n",
    "  #3.3 Fit a model on the training set and evaluate it on the test set\n",
    "  model <- lm(cbind(h1n1_vaccine, seasonal_vaccine) ~ ., data=train_data)\n",
    "  Y_pred <- predict(model,test_data[,-targets])\n",
    "  Y_h1n1 <- test_data[,targets[2]]\n",
    "  Y_seasonal <- test_data[,targets[1]]\n",
    "  \n",
    "  #3.4 Store the prediction of the tree (2 is to take only the P(Y=\"spam\"|x))\n",
    "  Y_hat <- ifelse(Y_pred > threshold,1,0) \n",
    "  # Need one confusion matrix for h1n1 and one for seasonal\n",
    "  confusion_matrix_h1n1 <- table(Y_hat[,1],Y_h1n1)\n",
    "  confusion_matrix_seasonal <- table(Y_hat[,2],Y_seasonal)\n",
    "  \n",
    "  #3.5 Retain the evaluation score and discard the model\n",
    "  accuracy_vec_h1n1[i] = (confusion_matrix_h1n1[1,1]+confusion_matrix_h1n1[2,2])/sum(confusion_matrix_h1n1)\n",
    "  misclassification_rate = 1 - accuracy_vec_h1n1[i]\n",
    "  print(paste(\"[INFO] - Misclassification rate h1n1 -\",i,\"fold:\",misclassification_rate))\n",
    "  \n",
    "  accuracy_vec_seasonal[i] = (confusion_matrix_seasonal[1,1]+confusion_matrix_seasonal[2,2])/sum(confusion_matrix_seasonal)\n",
    "  misclassification_rate = 1 - accuracy_vec_seasonal[i]\n",
    "  print(paste(\"[INFO] - Misclassification rate seasonal -\",i,\"fold:\",misclassification_rate))\n",
    "  \n",
    "}\n",
    "\n",
    "#4. Summarize the skill of the model using the sample of model evaluation scores\n",
    "print(paste(\"[INFO] - Mean misclassification rate h1n1:\",1-mean(accuracy_vec_h1n1)))\n",
    "print(paste(\"[INFO] - Mean misclassification rate seasonal:\",1-mean(accuracy_vec_seasonal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the misclassification rate for each of the targets. The missclassification rate is not perfect but is enough to decide to use this model for the first try.\n",
    "Then, the ROC and AUC for each of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>1329</li><li>2</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1329\n",
       "\\item 2\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1329\n",
       "2. 2\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1329    2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NULL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ERROR",
     "evalue": "Error in table(Y_hat, Y): all arguments must have the same length\n",
     "output_type": "error",
     "traceback": [
      "Error in table(Y_hat, Y): all arguments must have the same length\nTraceback:\n",
      "1. AUCROC(Y_pred, test_data[, targets[1]], \"ROC Curve for seasonal vaccine\")",
      "2. table(Y_hat, Y)   # at line 8 of file <text>",
      "3. stop(\"all arguments must have the same length\")"
     ]
    }
   ],
   "source": [
    "# ROC for seasonal vaccine\n",
    "dim(Y_pred)\n",
    "dim(test_data[,targets[1]])\n",
    "AUC_seasonal = AUCROC(Y_pred, test_data[,targets[1]],\"ROC Curve for seasonal vaccine\")\n",
    "\n",
    "# ROC for h1n1 vaccine\n",
    "\n",
    "AUC_h1n1 =  AUCROC(Y_pred, test_data[,targets[2]],\"ROC Curve for h1n1 vaccine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the linear model is validated, it can be used to predict the outcome with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- lm(cbind(h1n1_vaccine, seasonal_vaccine)~., data=total_train_set)\n",
    "summary(model)\n",
    "Y_pred <- predict(model,pca_ts_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary of the model gives a better understanding of how it works with 26607 lines and 54 features. In reality, the outputs are split in two to present one and then the other. The p-value is a good indicator of the quality of the model and the features selected. Indeed, for the h1n1 vaccine, 32 features have a p-value indicating that they are statistically highly significant. For the seasonal vaccine, 37 features are in this case. The AUC of the seasonal vaccine is indeed higher. In all cases, the summary confirms that the chosen linear model is not the best one but works quite well for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "Then we use the nnet package to make single layer neural network.\n",
    "\n",
    "Let's define a function to compute neural networks effectiveness based on the number of hidden notes using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_nnet_cross_valid <- function(X, Y, hidden_nodes=10,k=5){\n",
    "    # Targe 0 for h1n1, 1 for seasonal\n",
    "    accuracy_vec <- c()\n",
    "\n",
    "    # 1. Shuffle the dataset randomly.\n",
    "    shuffled_idx <- sample(1:nrow(X))\n",
    "    half_split <- floor(nrow(X)/2)\n",
    "\n",
    "    # 2. Split the dataset into k groups\n",
    "    max <- ceiling(nrow(X)/k)\n",
    "    splits <- split(shuffled_idx, ceiling(seq_along(shuffled_idx)/max))\n",
    "    \n",
    "    # 3. For each unique group:\n",
    "    for (i in 1:k){\n",
    "        #3.1 Take the group as a hold out or test data set\n",
    "        X_test <- X[splits[[i]],]\n",
    "        Y_test <- Y[splits[[i]]]\n",
    "        \n",
    "        #3.2 Take the remaining groups as a training data set\n",
    "        X_train <- X[-splits[[i]],]\n",
    "        Y_train <- Y[-splits[[i]]]\n",
    "        # print(paste(\"[INFO] - Training set size:\",dim(train_data)[1],\"- Testing set size\",dim(test_data)[1]))\n",
    "        \n",
    "        #3.3 Fit a model on the training set and evaluate it on the test set\n",
    "        model <- nnet(x=X,\n",
    "                     y=Y,\n",
    "                     size=hidden_nodes,\n",
    "                     skip=FALSE, maxit=1000,rang=1,MaxNWts=10000,trace=FALSE)\n",
    "        print(paste(\"reached maxit ?\",model$convergence))\n",
    "        Y_pred<-predict(model,X_test)\n",
    "        \n",
    "        #3.4 Retain the evaluation score and discard the model\n",
    "        accuracy_vec[i] <- compute_AUC(Y_pred, Y_test)\n",
    "        break\n",
    "    }\n",
    "    \n",
    "    return(mean(accuracy_vec))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] - Testing h= 10\"\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in auc_nnet_cross_valid(pca_tr_set, label_1, hidden_nodes = hidden_nodes, : could not find function \"auc_nnet_cross_valid\"\n",
     "output_type": "error",
     "traceback": [
      "Error in auc_nnet_cross_valid(pca_tr_set, label_1, hidden_nodes = hidden_nodes, : could not find function \"auc_nnet_cross_valid\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "label_1 <- training_set_labels[,\"h1n1_vaccine\"]\n",
    "label_2 <- training_set_labels[, \"seasonal_vaccine\"]\n",
    "labels <- cbind(h1n1_vaccine=label_1, seasonal_vaccine=label_2)\n",
    "\n",
    "\n",
    "k <- 5\n",
    "\n",
    "hidden_nodes_vec <- seq(10, 70, 10)\n",
    "\n",
    "results_h1n1 <- c()\n",
    "results_season <- c()\n",
    "\n",
    "for(hidden_nodes in hidden_nodes_vec){\n",
    "    print(paste(\"[INFO] - Testing h=\",hidden_nodes))\n",
    "    results_h1n1 <- c(results_h1n1, auc_nnet_cross_valid(pca_tr_set, label_1, hidden_nodes=hidden_nodes,k=k))\n",
    "    results_season <- c(results_season, auc_nnet_cross_valid(pca_tr_set, label_é, hidden_nodes=hidden_nodes,k=k))\n",
    "}\n",
    "# mean(AUC) according to the nb of hidden nodes \n",
    "par(mfrow=c(1,2))\n",
    "\n",
    "plot(hidden_nodes_vec, results_h1n1)\n",
    "lines(hidden_nodes_vec, results_h1n1)\n",
    "\n",
    "plot(hidden_nodes_vec, results_season)\n",
    "lines(hidden_nodes_vec, results_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results are much better when predicting the seasonal vaccines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third model chosen is the SVM (Support Vector Machine), using the e1071 package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"e1071\")\n",
    "#prediction for H1N1 column\n",
    "data_svm1 = svm(label_1 ~ ., data = pca_tr_set, kernel = \"polynomial\",degree=3, cost = 1, scale = FALSE)\n",
    "Y_pred1 <- predict(data_svm1,pca_ts_set)\n",
    "ss <- preProcess(as.data.frame(Y_pred1), method=c(\"range\"))\n",
    "Y_pred1 <- predict(ss, as.data.frame(Y_pred1))\n",
    "\n",
    "#prediction for Seasonal Flu column\n",
    "data_svm2 = svm(label_2 ~ ., data = pca_tr_set, kernel = \"polynomial\",degree=3, cost = 1, scale = FALSE)\n",
    "Y_pred2 <- predict(data_svm2,pca_ts_set)\n",
    "ss <- preProcess(as.data.frame(Y_pred2), method=c(\"range\"))\n",
    "Y_pred2 <- predict(ss, as.data.frame(Y_pred2))\n",
    "\n",
    "final_pred_SVM <- cbind(testing_features[, c(1)], Y_pred1, Y_pred2)\n",
    "write.csv(final_pred_SVM, \"test_submission_SVM.csv\", row.names = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function \"svm()\" can take a large amount of  different arguments,like the kernel type and its parameters. The arguments chosen in the code were selected after testing multiple combinations by computing their ROC and the AUC. Those results are shown in the following table. If some parameters are not specified hereunder, it means that the default values were used. Some values of the AUC for the \"seasonal flue vaccines\" are also missing because in view of the bad results of the \"H1N1\" column for this line, it was not deemed necessary to compute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| cost   | kernel       | gamma | coef0 | AUC H1N1 | AUC Flue |\n",
    "|--------|--------------|-------|-------|----------|----------|\n",
    "|0,00001 | linear       |       |       |   0,804  |   0,738  |\n",
    "|0,00001 | polynomial 2 |       |       |   0,804  |   0,739  |\n",
    "|0,00001 | polynomial 3 |       |       |   0,840  |   0,732  |\n",
    "|0,00001 | polynomial 3 | 0,0001|   5   |   0,812  |   0,732  |\n",
    "|0,000001| polynomial 3 |       |       |   0,813  |          |\n",
    "|0,00001 | polynomial 4 |       |       |   0,811  |   0,725  |\n",
    "|0,1     | polynomial 3 |       |       |   0,849  |   0,729  |\n",
    "|0,01    | polynomial 3 |       |       |   0,844  |          |\n",
    "|10      | polynomial 3 |       |       |   0,840  |   0,732  |\n",
    "|1       | polynomial 3 |       |       |   0,849  |   0,739  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative models\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
