{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO-F-422 -  Statistical Foundations of Machine Learning \n",
    "\n",
    "### Alexandre Flachs - __[alexandre.flachs@ulb.be](mailto:alexandre.flachs@ulb.be) - Student ID 474748__\n",
    "### Marie Giot - __[marie.giot@ulb.be](mailto:marie.giot@ulb.be) - Student ID 474915__\n",
    "### Jeanne Szpirer - __[jeanne.szpirer@ulb.be](mailto:jeanne.szpirer@ulb.be) - Student ID 477286__\n",
    "\n",
    "### Video presentation: www.youtube.com/abcd1234\n",
    "\n",
    "## Flu Shot Learning: Predict H1N1 and Seasonal Flu Vaccines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project's goal is to propose efficient machine learning methods to predict the likelihood of patients to get a vaccine for H1N1 or Seasonal Flu. This challenge was inititated by the \"Driven Data\" plateforme. Four different techniques are presented hereunder and their costs and their results will be discussed. The group also submitted its result on the Driven Data plateforme for the challenge in order to receive a score and compare it to the other paticipants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Before working any model we need to preprocess the data to make it usefull. This pipeline in divided intro three parts :\n",
    "1. **Missing value imputation** : Replace missing values, possibly using other known values\n",
    "2. **Feature engineering** : Define useful features from available ones. \n",
    "3. **Feature selection** : Some features might be useless or give wrong indications to the model, we might need to remove some features.\n",
    "\n",
    "Let's start by importing our data, then develop each of the above parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set features\n",
    "training_set_features <- read.csv(\"training_set_features.csv\", stringsAsFactors = T, na.strings = c(\"NA\", \"\"))\n",
    "dim(training_set_features)\n",
    "\n",
    "# Test set features\n",
    "test_set_features <- read.csv(\"test_set_features.csv\", stringsAsFactors = T, na.strings = c(\"NA\", \"\"))\n",
    "dim(test_set_features)\n",
    "\n",
    "# Training set labels\n",
    "training_set_labels <- read.csv(\"training_set_labels.csv\", stringsAsFactors = T, na.strings = c(\"NA\", \"\"))\n",
    "dim(training_set_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training features set and the training labels set has the same amount of lines, this is a first good sign because it means that we have an \"answer\" for every training line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We summarize our data before doing any work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(training_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have many missing values, in most features. We can compare the number of lines left if we remove any line containing any missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First method\n",
    "cat(\"Training set : \", dim(training_set_features)[1], \"->\", dim(na.omit(training_set_features))[1], \"\\n\")\n",
    "cat(\"Test set     : \", dim(test_set_features)[1], \"->\", dim(na.omit(test_set_features))[1], \"\\n\")\n",
    "cat(\"Training labs: \", dim(training_set_labels)[1], \"->\", dim(na.omit(training_set_labels))[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least no line from the training labels misses any value, we can thus use every entry from the training set for both targets.\n",
    "Counting the number of missing value per feature allows us to see if some of the could be useless. The health insurance, employment occupation and employment industry lines are the emptiest (almost half of the lines miss this data) but by intuition this might be a huge factor in the vaccination decision so we keep it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut aussi regarder si certaines colonnes n'ont vraiment quasi aucune valeur, dans ce cas, Ã§a vaut pas vraiment la peine de garder\n",
    "a <- sapply(training_set_features, function(x) sum(is.na(x)))\n",
    "print(a[order(a, decreasing=T)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we remove these fields ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_nas <- which(sapply(test_set_features, function(x) sum(is.na(x))) > 10000)\n",
    "\n",
    "cat(\"Training set : \",\n",
    "    dim(training_set_features[,-useful_nas])[1], \"->\",\n",
    "    dim(na.omit(training_set_features[,-useful_nas]))[1], \"\\n\")\n",
    "cat(\"Test set     : \", dim(test_set_features[,-useful_nas])[1], \"->\",\n",
    "    dim(na.omit(test_set_features[,-useful_nas]))[1], \"\\n\")\n",
    "cat(\"Training labels: \", dim(training_set_labels)[1], \"->\", dim(na.omit(training_set_labels))[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are much less lines containing missing values. We will thus manage the three most missed fields differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of missing numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation will be the same for training and testing so we merge them\n",
    "features_set <- rbind(training_set_features, test_set_features)\n",
    "\n",
    "\n",
    "# Note the indexes of each dataset to get them back after\n",
    "tr_indexes <- 1:nrow(training_set_features)\n",
    "ts_indexes <- (nrow(training_set_features)+1):(nrow(training_set_features) + nrow(test_set_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer encoding of some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels(features_set[, \"age_group\"])\n",
    "levels(features_set[, \"age_group\"]) <- 0:4\n",
    "features_set[, \"age_group\"] <- as.numeric(features_set[, \"age_group\"])\n",
    "features_set[, \"age_group\"] <- (features_set[, \"age_group\"] - 1)/4\n",
    "\n",
    "levels(features_set[, \"education\"])\n",
    "levels(features_set[, \"education\"]) <- 0:3\n",
    "features_set[, \"education\"] <- as.numeric(features_set[, \"education\"])\n",
    "features_set[, \"education\"] <- (features_set[, \"education\"] - 1)/3\n",
    "\n",
    "levels(features_set[, \"income_poverty\"])\n",
    "levels(features_set[, \"income_poverty\"]) <- c(1, 2, 0)\n",
    "features_set[, \"income_poverty\"] <- as.numeric(features_set[, \"income_poverty\"])\n",
    "features_set[, \"income_poverty\"] <- (features_set[, \"income_poverty\"] - 1)/2\n",
    "\n",
    "levels(features_set[, \"census_msa\"])\n",
    "levels(features_set[, \"census_msa\"]) <- c(1, 2, 0)\n",
    "features_set[, \"census_msa\"] <- as.numeric(features_set[, \"census_msa\"])\n",
    "features_set[, \"census_msa\"] <- (features_set[, \"census_msa\"] - 1)/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non negative matrix factorization based imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non negative matrix factorization is a procedure which consists of approximating a matrix with non negative values as the product of two matrices having smaller dimensions. More formally, a matrix $M$ of dimension $m\\times n$ is decomposed into two matrices $W$ and $H$ of dimensions $m\\times p$ and $p\\times n$ respectively such that $M \\approx WH$.\n",
    "\n",
    "More details concerning the imputation method can be found here https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8510447/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PseudoCode :\n",
    "\n",
    "```pseudocode\n",
    "Consider M of dim m x n having missing values\n",
    "Fix N > 0\n",
    "Normalize M and replace NAs using mean imputation\n",
    "k1 <- floor(max(abs(rank(M) - N/2), 1))\n",
    "kN = min(abs(rX + N/2), m, n)\n",
    "\n",
    "# Compute approximation\n",
    "initialize X array of size kN-k1+1\n",
    "for k in 1...kN-k1+1 do\n",
    "    Compute the nNMF of M on non missing values and store the result in X[k]\n",
    "\n",
    "## Based on all X[k], reconstruct M\n",
    "# Weights based only on non missing values of M\n",
    "initialize d array of size kN-k1+1\n",
    "for k in 1...kN-k1+1 do\n",
    "    Compute d[k] = sum(X_ij - M_ij)/ nb_nonNA\n",
    "\n",
    "# Reconstruct\n",
    "Initialize divisor=0, M_hat of 0 with same dim as M\n",
    "for k in 1...kN-k1+1 do\n",
    "    M_hat += exp(-d[k])*X[k]\n",
    "    denum += exp(-d[k])\n",
    "M_hat <- M_hat/denum\n",
    "```\n",
    "\n",
    "\n",
    "After these steps, M_hat does not contain missing values and the imputation is based on local and global information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(NMFN)\n",
    "\n",
    "# We work only on the columns of numeric type\n",
    "numeric_variables_idx<-which(sapply(features_set[1,],class)!=\"factor\")\n",
    "numeric_variables_idx <- numeric_variables_idx[-c(1, 16)] # Remove resp ID and health insur\n",
    "\n",
    "# We need to normalize this data to use efficient nNMF\n",
    "library(\"caret\")\n",
    "\n",
    "ss <- preProcess(as.data.frame(features_set[, numeric_variables_idx]), method=c(\"range\"))\n",
    "features_set[,numeric_variables_idx] <- predict(ss, as.data.frame(features_set[, numeric_variables_idx]))\n",
    "summary(features_set[, numeric_variables_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that will compute nmf\n",
    "nfm_mult_upd <- function(R, K, missing_idx, maxit=800, eps=2.2204e-16) {\n",
    "    # Using weighted multiplicative rule Zhu 2016\n",
    "    # init random W and H\n",
    "    print(paste(\"[INFO] : NMF with k=\", K))\n",
    "    R <- as.matrix(R)\n",
    "    I <- dim(R)[1]\n",
    "    J <- dim(R)[2]\n",
    "    M <- matrix(1, nrow = dim(R)[1], ncol = dim(R)[2])\n",
    "    M[missing_idx] <- 0\n",
    "    X <- R # Store original R\n",
    "    R <- R*M\n",
    "    W <- matrix(runif(I*K), nrow = I, ncol = K)\n",
    "    H <- matrix(runif(K*J), nrow = K, ncol = J)\n",
    "    \n",
    "    n <- 0\n",
    "    d1 <- 1000\n",
    "    d2 <- 1000\n",
    "    while(n < maxit && !(d1 < eps && d2 < eps)) {\n",
    "        if (n %% 100 == 0) {\n",
    "            print(paste(\"[INFO] : iter\", n, \" Relative error is :\", distance2(X, W%*%H)/distance2(X, R*0)))\n",
    "        }\n",
    "        newH <- H* (t(W) %*% R) / (t(W) %*% W %*% H)\n",
    "        newW <- W*(R %*% t(newH)) / ((W %*% newH) %*% t(newH))\n",
    "        \n",
    "        d1 <- distance2(newH, H)\n",
    "        d2 <- distance2(newW, W)\n",
    "        \n",
    "        H <- newH\n",
    "        W <- newW\n",
    "        n <- n+1\n",
    "    }\n",
    "    \n",
    "    Res <- W%*%H\n",
    "    #Res[missing_idx] <- X[missing_idx]\n",
    "    nmf <- list(\"res\"=Res, \"dst\"=distance2(R, Res)/distance2(R, 0))\n",
    "    return(nmf)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: initialize by defining N and replace NAs by mean\n",
    "N <- 4\n",
    "\n",
    "replace_na_with_mean_value<-function(vec) {\n",
    "    mean_vec <- mean(as.numeric(vec), na.rm=TRUE)\n",
    "    vec[is.na(vec)]<-mean_vec\n",
    "    vec\n",
    "}\n",
    "\n",
    "X<-data.frame(apply(features_set[, numeric_variables_idx], MARGIN=2, replace_na_with_mean_value))\n",
    "miss_idx = which(is.na(features_set[,numeric_variables_idx]), arr.ind = T)\n",
    "non_miss_idx <- which(!is.na(features_set[,numeric_variables_idx]), arr.ind = T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: nNMF for k=k1 to K=kN with masking missing values\n",
    "library(Matrix) # For rankMatrix \n",
    "rX <- rankMatrix(X)\n",
    "k1 = floor(max(abs(rX - N/2), 1))\n",
    "kN = min(abs(rX + N/2), dim(X)[1], dim(X)[2])\n",
    "\n",
    "X_hat = array(0, dim = c(kN-k1+1, nrow(X), ncol(X)))\n",
    "dim(X_hat)\n",
    "for (K in k1:kN) {\n",
    "    # compute NMF\n",
    "    nmf <- nfm_mult_upd(X, K, missing_idx = miss_idx, maxit=800)\n",
    "    print(paste(\"Computed nmf, final dist is\", nmf$dst))\n",
    "    X_hat[K-k1+1,,] <- nmf$res\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: weighted reconstruction\n",
    "d = array(0, dim = c(kN-k1+1))\n",
    "for (K in 1:(kN-k1+1)) {\n",
    "    # Reconstruction error based on non missing values\n",
    "    d[K] <- sum(abs(X_hat[K,,][non_miss_idx] - X[non_miss_idx]))/nrow(non_miss_idx)\n",
    "}\n",
    "\n",
    "X_hat_f <- matrix(0, nrow = nrow(X), ncol = ncol(X))\n",
    "denum <- 0\n",
    "for (K in 1:(kN-k1+1)) {\n",
    "    # Reconstruction matrix\n",
    "    X_hat_f <- X_hat_f + exp(-d[K])*X_hat[K,,]\n",
    "    denum <- denum + exp(-d[K])\n",
    "}\n",
    "X_hat_f <- X_hat_f / sum(exp(-d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace in the feature set\n",
    "X[miss_idx] <- X_hat_f[miss_idx]\n",
    "features_set[, numeric_variables_idx] <- X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we managed numerical variabels we can work on encoding the categorical ones which we did not work on yet.\n",
    "\n",
    "Getting back to the columns containing many missing values, we use one hot encoding and create a category \"missing\", considering that not having this piece of information could be meaning something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_nas <- which(sapply(test_set_features, function(x) sum(is.na(x))) > 10000)\n",
    "\n",
    "library(fastDummies)\n",
    "features_set <- dummy_cols(features_set, \n",
    "                           select_columns = names(useful_nas),\n",
    "                           remove_selected_columns = T)\n",
    "\n",
    "## Change NAs to 0 for the new one hot encoded columns\n",
    "replace_na_with_0<-function(vec) {\n",
    "    vec[is.na(vec)]<-0\n",
    "    vec\n",
    "}\n",
    "\n",
    "useful_nas_feat_idx <- c(grep(\"health_insurance_*\", colnames(features_set)))\n",
    "useful_nas_feat_idx <- c(useful_nas_feat_idx, grep(\"employment_industry*\", colnames(features_set)))\n",
    "useful_nas_feat_idx <- c(useful_nas_feat_idx, grep(\"employment_occup*\", colnames(features_set)))\n",
    "\n",
    "features_set[,useful_nas_feat_idx] <- replace_na_with_0(features_set[,useful_nas_feat_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining columns will be treated the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_variables<-which(sapply(features_set[1,],class)==\"factor\")\n",
    "factor_variables\n",
    "factor_col_names <- colnames(factor_variables)\n",
    "\n",
    "features_set_int_encoded <- dummy_cols(features_set,\n",
    "                                       select_columns = factor_col_names,\n",
    "                                       ignore_na = F,\n",
    "                                       remove_selected_columns = T)\n",
    "\n",
    "\n",
    "features_set_int_encoded <- replace_na_with_0(features_set_int_encoded)\n",
    "\n",
    "tr_set_enc <- features_set_int_encoded[tr_indexes,]\n",
    "ts_set_enc <- features_set_int_encoded[ts_indexes,]\n",
    "\n",
    "summary(features_set_int_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA was used to select the best features. This technique creates new fewer but more representative features from the current set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features <- tr_set_enc[, -c(1)]\n",
    "summary(training_features)\n",
    "training_labels <- training_set_labels\n",
    "label_1 <- training_labels[,\"h1n1_vaccine\"]\n",
    "label_2 <- training_labels[, \"seasonal_vaccine\"]\n",
    "labels <- cbind(h1n1_vaccine=label_1, seasonal_vaccine=label_2)\n",
    "\n",
    "testing_features <- ts_set_enc[, -c(1)]\n",
    "feature_set <- rbind(training_features, testing_features)\n",
    "feature_set <- feature_set[, -c(1)] # remove resp id\n",
    "pca <- prcomp(feature_set)\n",
    "summary(pca)\n",
    "\n",
    "pca_feat_set <- pca$x[,1:54]\n",
    "\n",
    "library(\"caret\")\n",
    "\n",
    "ss <- preProcess(as.data.frame(pca_feat_set), method=c(\"range\"))\n",
    "pca_feat_set <- predict(ss, as.data.frame(pca_feat_set))\n",
    "\n",
    "pca_tr_set <- pca_feat_set[1:26707,]\n",
    "pca_ts_set <- pca_feat_set[26708:53415,]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the 54 features was made on the basis of the analysis of the pca summary. Indeed, the standard deviations and variances allowed us to conclude that 54 features were sufficient to train a model. The cumulative proportion also shows that after taking 54 features, the problem was defined at a rate of 95% which seems enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first thing to define is the way in which the models will be compared and evaluated. To fit with the way the Driven Data plateforne is evaluating scores, the ROC and AUC are used to estimate the efficienty of a method. Those two mesures are computed with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROC <- function(Y_pred, Y, title_of_curve) {\n",
    "    thresholds <- seq(0,0.99,0.05)\n",
    "    FPR <- c()\n",
    "    TPR <- c()\n",
    "\n",
    "    for(threshold in thresholds){\n",
    "      Y_hat <- ifelse(Y_pred > threshold,1,0) \n",
    "      confusion_matrix <- table(Y_hat,Y)\n",
    "\n",
    "      if(dim(confusion_matrix)[1] < 2){ \n",
    "        if(rownames(confusion_matrix) == 0){\n",
    "          confusion_matrix <- rbind(confusion_matrix,c(0,0))\n",
    "          rownames(confusion_matrix)[2] <- 1\n",
    "        }\n",
    "        if(rownames(confusion_matrix) == 1){\n",
    "          confusion_matrix <- rbind(c(0,0),confusion_matrix)\n",
    "          rownames(confusion_matrix)[1] <- 0\n",
    "        }\n",
    "      }\n",
    "\n",
    "      FP <- confusion_matrix[2,1]\n",
    "      TP <- confusion_matrix[2,2]\n",
    "      N_N <- sum(confusion_matrix[,1]) # Total number of 0's\n",
    "      N_P <- sum(confusion_matrix[,2]) # Total number of 1's\n",
    "\n",
    "      FPR <- c(FPR,FP/N_N)\n",
    "      TPR <- c(TPR,TP/N_P)\n",
    "    }\n",
    "    FPR <- c(1, FPR, 0)\n",
    "    TPR <- c(1, TPR, 0)\n",
    "    plot.new()\n",
    "    plot(FPR,TPR)\n",
    "    lines(FPR,TPR,col=\"blue\")\n",
    "    lines(thresholds,thresholds,lty=2)\n",
    "    title(title_of_curve)\n",
    "    AUC <- sum(abs(diff(FPR)) * (head(TPR,-1)+tail(TPR,-1)))/2\n",
    "    return (AUC)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "First the linear model is going to be used. It is one the fastest model to train and it will give a good idea of the kind of relation that exists between features and labels. The only difference with the model viewed in practicals is that two outputs are predicted at a time, it means that the train set needs to have the two labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_set <- cbind(pca_tr_set, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making the prediction for the given test values, it is important to check that the model fits by doing a cross-validation and calculating the AUC and ROC. First, the cross-validation with the multivariate linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation of the train in two groups in order to calculate the error between prediction and reality.\n",
    "vaccine_idx <- sample(1:nrow(total_train_set))\n",
    "half_split <- floor(nrow(total_train_set)/2)\n",
    "train_data_set <- total_train_set[vaccine_idx[1:half_split],]\n",
    "test_data <- total_train_set[vaccine_idx[(half_split+1):nrow(total_train_set)],]\n",
    "target_idx <- ncol(train_data_set)\n",
    "targets <- c(target_idx, target_idx-1)\n",
    "\n",
    "# Linear model & prediction\n",
    "model <- lm(cbind(h1n1_vaccine, seasonal_vaccine)~., data=train_data_set)\n",
    "Y_pred <- predict(model,test_data[,-targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "accuracy_vec_h1n1 <- array(0,k)\n",
    "accuracy_vec_seasonal <- array(0,k)\n",
    "threshold <- 0.5\n",
    "\n",
    "# 1. Shuffle the dataset randomly.\n",
    "vaccine_idx <- sample(1:nrow(train_data_set))\n",
    "\n",
    "# 2. Split the dataset into k groups\n",
    "max <- ceiling(nrow(train_data_set)/k)\n",
    "splits <- split(vaccine_idx, ceiling(seq_along(vaccine_idx)/max))\n",
    "\n",
    "# 3. For each unique group:\n",
    "for (i in 1:k){\n",
    "  #3.1 Take the group as a hold out or test data set\n",
    "  test_data <- train_data_set[splits[[i]],]\n",
    "  \n",
    "  #3.2 Take the remaining groups as a training data set\n",
    "  train_data <- train_data_set[-splits[[i]],]\n",
    "  print(paste(\"[INFO] - Training set size:\",dim(train_data)[1],\"- Testing set size\",dim(test_data)[1]))\n",
    "  \n",
    "  #3.3 Fit a model on the training set and evaluate it on the test set\n",
    "  model <- lm(cbind(h1n1_vaccine, seasonal_vaccine) ~ ., data=train_data)\n",
    "  Y_pred <- predict(model,test_data[,-targets])\n",
    "  Y_h1n1 <- test_data[,targets[2]]\n",
    "  Y_seasonal <- test_data[,targets[1]]\n",
    "  \n",
    "  #3.4 Store the prediction of the tree (2 is to take only the P(Y=\"spam\"|x))\n",
    "  Y_hat <- ifelse(Y_pred > threshold,1,0) \n",
    "  # Need one confusion matrix for h1n1 and one for seasonal\n",
    "  confusion_matrix_h1n1 <- table(Y_hat[,1],Y_h1n1)\n",
    "  confusion_matrix_seasonal <- table(Y_hat[,2],Y_seasonal)\n",
    "  \n",
    "  #3.5 Retain the evaluation score and discard the model\n",
    "  accuracy_vec_h1n1[i] = (confusion_matrix_h1n1[1,1]+confusion_matrix_h1n1[2,2])/sum(confusion_matrix_h1n1)\n",
    "  misclassification_rate = 1 - accuracy_vec_h1n1[i]\n",
    "  print(paste(\"[INFO] - Misclassification rate h1n1 -\",i,\"fold:\",misclassification_rate))\n",
    "  \n",
    "  accuracy_vec_seasonal[i] = (confusion_matrix_seasonal[1,1]+confusion_matrix_seasonal[2,2])/sum(confusion_matrix_seasonal)\n",
    "  misclassification_rate = 1 - accuracy_vec_seasonal[i]\n",
    "  print(paste(\"[INFO] - Misclassification rate seasonal -\",i,\"fold:\",misclassification_rate))\n",
    "  \n",
    "}\n",
    "\n",
    "#4. Summarize the skill of the model using the sample of model evaluation scores\n",
    "print(paste(\"[INFO] - Mean misclassification rate h1n1:\",1-mean(accuracy_vec_h1n1)))\n",
    "print(paste(\"[INFO] - Mean misclassification rate seasonal:\",1-mean(accuracy_vec_seasonal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the misclassification rate for each of the targets. The missclassification rate is not perfect but is enough to decide to use this model for the first try.\n",
    "Then, the ROC and AUC for each of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC for seasonal vaccine\n",
    "\n",
    "AUC_seasonal = AUCROC(Y_pred[,2], test_data[,targets[1]],\"ROC Curve for seasonal vaccine\")\n",
    "\n",
    "# ROC for h1n1 vaccine\n",
    "\n",
    "AUC_h1n1 =  AUCROC(Y_pred[,1], test_data[,targets[2]],\"ROC Curve for h1n1 vaccine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the linear model is validated, it can be used to predict the outcome with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- lm(cbind(h1n1_vaccine, seasonal_vaccine)~., data=total_train_set)\n",
    "summary(model)\n",
    "Y_pred <- predict(model,pca_ts_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary of the model gives a better understanding of how it works with 26607 lines and 54 features. In reality, the outputs are split in two to present one and then the other. The p-value is a good indicator of the quality of the model and the features selected. Indeed, for the h1n1 vaccine, 32 features have a p-value indicating that they are statistically highly significant. For the seasonal vaccine, 37 features are in this case. The AUC of the seasonal vaccine is indeed higher. In all cases, the summary confirms that the chosen linear model is not the best one but works quite well for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "Then we use the nnet package to make another model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third model chosen is the SVM (Support Vector Machine), using the e1071 package. This method has usually good results even for data sets with many dimensions, which will be practical with the 54 features set selected by the PCA. The kernel and its parameters are also easily modular which allows us to test multiple combinations in order to find the one with the better results. The fact that the kernel can take other types then the linear one (polynomial, sigmoid or radial) also allows to evaluate efficiently data that are difficult to separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"e1071\")\n",
    "#prediction for H1N1 column\n",
    "data_svm1 = svm(label_1 ~ ., data = pca_tr_set, kernel = \"polynomial\",degree=3, cost = 1, scale = FALSE)\n",
    "Y_pred1 <- predict(data_svm1,pca_ts_set)\n",
    "ss <- preProcess(as.data.frame(Y_pred1), method=c(\"range\"))\n",
    "Y_pred1 <- predict(ss, as.data.frame(Y_pred1))\n",
    "\n",
    "#prediction for Seasonal Flu column\n",
    "data_svm2 = svm(label_2 ~ ., data = pca_tr_set, kernel = \"polynomial\",degree=3, cost = 1, scale = FALSE)\n",
    "Y_pred2 <- predict(data_svm2,pca_ts_set)\n",
    "ss <- preProcess(as.data.frame(Y_pred2), method=c(\"range\"))\n",
    "Y_pred2 <- predict(ss, as.data.frame(Y_pred2))\n",
    "\n",
    "final_pred_SVM <- cbind(testing_features[, c(1)], Y_pred1, Y_pred2)\n",
    "write.csv(final_pred_SVM, \"test_submission_SVM.csv\", row.names = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function \"svm()\" can take a large amount of  different arguments,like the kernel type and its parameters. The arguments chosen in the code were selected after testing multiple combinations by computing their ROC and the AUC. Those results are shown in the following table. If some parameters are not specified hereunder, it means that the default values were used. Some values of the AUC for the \"seasonal flue vaccines\" are also missing because in view of the bad results of the \"H1N1\" column for this line, it was not deemed necessary to compute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| cost   | kernel       | gamma | coef0 | AUC H1N1 | AUC Flue |\n",
    "|--------|--------------|-------|-------|----------|----------|\n",
    "|0,00001 | linear       |       |       |   0,804  |   0,738  |\n",
    "|0,00001 | polynomial 2 |       |       |   0,804  |   0,739  |\n",
    "|0,00001 | polynomial 3 |       |       |   0,840  |   0,732  |\n",
    "|0,00001 | polynomial 3 | 0,0001|   5   |   0,812  |   0,732  |\n",
    "|0,000001| polynomial 3 |       |       |   0,813  |          |\n",
    "|0,00001 | polynomial 4 |       |       |   0,811  |   0,725  |\n",
    "|0,1     | polynomial 3 |       |       |   0,849  |   0,729  |\n",
    "|0,01    | polynomial 3 |       |       |   0,844  |          |\n",
    "|10      | polynomial 3 |       |       |   0,840  |   0,732  |\n",
    "|1       | polynomial 3 |       |       |   0,849  |   0,739  |\n",
    "|0,01    | radial       |       |       |   0,849  |   0,735  |\n",
    "|0,1     | radial       |       |       |   0,849  |   0,741  |\n",
    "|1       | radial       |       |       |   0,839  |   0,733  |\n",
    "|10      | radial       |       |       |   0,842  |   0,733  |\n",
    "|100     | radial       |       |       |   0,849  |   0,730  |\n",
    "|0,01    | sigmoid      |       |       |   0,848  |   0,736  |\n",
    "|0,1     | sigmoid      |       |       |   0,849  |   0,740  |\n",
    "|1       | sigmoid      |       |       |   0,839  |   0,734  |\n",
    "|10      | sigmoid      |       |       |   0,838  |   0,731  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative models\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MRMR features selection method is chosen as an alternative to the PCA, to implement a method without using a library contained in the guidelines' list. This selection is the Minimum redundancy maximal relevancy filter (MRMR). It allows to have less features than the PCA but with a prediction that is normally just as good, maybe even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(praznik)\n",
    "library(praznik)\n",
    "\n",
    "test_set <- read.csv(\"ts_set_imputed.csv\")[, -c(1)]\n",
    "\n",
    "\n",
    "X <- read.csv(\"tr_set_imputed.csv\")[, -c(1)]\n",
    "Y <- read.csv(\"training_set_labels.csv\")\n",
    "summary(Y)\n",
    "\n",
    "results <- MRMR(X[, -c(1)], Y[, c(2)], 40, positive = T) # arbitrary number of cols to keep for h1n1\n",
    "results$selection\n",
    "results$score\n",
    "\n",
    "results <- MRMR(X[, -c(1)], Y[, c(3)], 40, positive = T) # arbitrary number of cols to keep for seasonal\n",
    "results$selection\n",
    "results$score\n",
    "\n",
    "features_to_keep = c(3,6,7,10,11,12,13,14,15,16,17,19,20,22,28,29,30,32,36,38,41,45,46,47,55,56,66,67,70,72,77,80,87,91,94)\n",
    "\n",
    "mrmr_tr_set <- X[, features_to_keep]\n",
    "mrmr_ts_set <- test_set[, features_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selection of features is made for one output and then the other. It remained to choose which features to keep from all those that emerged. 19 features were common to both labels so they were kept. For the h1n1_vaccine label, 8 additional features were selected but only 7 had a sufficient score to be considered interesting. For seasonal_vaccine, 13 additional features and 9 are interesting. In total there will be 35 features for both outputs. This is less than with the PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model with MRMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps are the same than with PCA, only the results will eventually change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_set <- cbind(mmr_tr_set, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation of the train in two groups in order to calculate the error between prediction and reality.\n",
    "vaccine_idx <- sample(1:nrow(total_train_set))\n",
    "half_split <- floor(nrow(total_train_set)/2)\n",
    "train_data_set <- total_train_set[vaccine_idx[1:half_split],]\n",
    "test_data <- total_train_set[vaccine_idx[(half_split+1):nrow(total_train_set)],]\n",
    "target_idx <- ncol(train_data_set)\n",
    "targets <- c(target_idx, target_idx-1)\n",
    "\n",
    "# Linear model & prediction\n",
    "model <- lm(cbind(h1n1_vaccine, seasonal_vaccine)~., data=train_data_set)\n",
    "Y_pred <- predict(model,test_data[,-targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "accuracy_vec_h1n1 <- array(0,k)\n",
    "accuracy_vec_seasonal <- array(0,k)\n",
    "threshold <- 0.5\n",
    "\n",
    "# 1. Shuffle the dataset randomly.\n",
    "vaccine_idx <- sample(1:nrow(train_data_set))\n",
    "\n",
    "# 2. Split the dataset into k groups\n",
    "max <- ceiling(nrow(train_data_set)/k)\n",
    "splits <- split(vaccine_idx, ceiling(seq_along(vaccine_idx)/max))\n",
    "\n",
    "# 3. For each unique group:\n",
    "for (i in 1:k){\n",
    "  #3.1 Take the group as a hold out or test data set\n",
    "  test_data <- train_data_set[splits[[i]],]\n",
    "  \n",
    "  #3.2 Take the remaining groups as a training data set\n",
    "  train_data <- train_data_set[-splits[[i]],]\n",
    "  print(paste(\"[INFO] - Training set size:\",dim(train_data)[1],\"- Testing set size\",dim(test_data)[1]))\n",
    "  \n",
    "  #3.3 Fit a model on the training set and evaluate it on the test set\n",
    "  model <- lm(cbind(h1n1_vaccine, seasonal_vaccine) ~ ., data=train_data)\n",
    "  Y_pred <- predict(model,test_data[,-targets])\n",
    "  Y_h1n1 <- test_data[,targets[2]]\n",
    "  Y_seasonal <- test_data[,targets[1]]\n",
    "  \n",
    "  #3.4 Store the prediction of the tree (2 is to take only the P(Y=\"spam\"|x))\n",
    "  Y_hat <- ifelse(Y_pred > threshold,1,0) \n",
    "  # Need one confusion matrix for h1n1 and one for seasonal\n",
    "  confusion_matrix_h1n1 <- table(Y_hat[,1],Y_h1n1)\n",
    "  confusion_matrix_seasonal <- table(Y_hat[,2],Y_seasonal)\n",
    "  \n",
    "  #3.5 Retain the evaluation score and discard the model\n",
    "  accuracy_vec_h1n1[i] = (confusion_matrix_h1n1[1,1]+confusion_matrix_h1n1[2,2])/sum(confusion_matrix_h1n1)\n",
    "  misclassification_rate = 1 - accuracy_vec_h1n1[i]\n",
    "  print(paste(\"[INFO] - Misclassification rate h1n1 -\",i,\"fold:\",misclassification_rate))\n",
    "  \n",
    "  accuracy_vec_seasonal[i] = (confusion_matrix_seasonal[1,1]+confusion_matrix_seasonal[2,2])/sum(confusion_matrix_seasonal)\n",
    "  misclassification_rate = 1 - accuracy_vec_seasonal[i]\n",
    "  print(paste(\"[INFO] - Misclassification rate seasonal -\",i,\"fold:\",misclassification_rate))\n",
    "  \n",
    "}\n",
    "\n",
    "#4. Summarize the skill of the model using the sample of model evaluation scores\n",
    "print(paste(\"[INFO] - Mean misclassification rate h1n1:\",1-mean(accuracy_vec_h1n1)))\n",
    "print(paste(\"[INFO] - Mean misclassification rate seasonal:\",1-mean(accuracy_vec_seasonal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC for seasonal vaccine\n",
    "\n",
    "AUC_seasonal = AUCROC(Y_pred[,2], test_data[,targets[1]],\"ROC Curve for seasonal vaccine\")\n",
    "\n",
    "# ROC for h1n1 vaccine\n",
    "\n",
    "AUC_h1n1 =  AUCROC(Y_pred[,1], test_data[,targets[2]],\"ROC Curve for h1n1 vaccine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC is better for seasonal_vaccine but worse for h1n1_vaccine. The linear model is therefore better suited with PCA to predict h1n1_vaccine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- lm(cbind(h1n1_vaccine, seasonal_vaccine)~., data=total_train_set)\n",
    "summary(model)\n",
    "Y_pred <- predict(model,pca_ts_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
