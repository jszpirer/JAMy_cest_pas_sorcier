{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO-F-422 -  Statistical Foundations of Machine Learning \n",
    "\n",
    "### Alexandre Flachs - __[alexandre.flachs@ulb.be](mailto:alexandre.flachs@ulb.be) - Student ID 474748__\n",
    "### Marie Giot - __[marie.giot@ulb.be](mailto:marie.giot@ulb.be) - Student ID 474915__\n",
    "### Jeanne Szpirer - __[jeanne.szpirer@ulb.be](mailto:jeanne.szpirer@ulb.be) - Student ID 477286__\n",
    "\n",
    "### Video presentation: www.youtube.com/abcd1234\n",
    "\n",
    "## Flu Shot Learning: Predict H1N1 and Seasonal Flu Vaccines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ajouter du texte d'intro avec jolies images ?* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Before working any model we need to preprocess the data to make it usefull. This pipeline in divided intro three parts :\n",
    "1. **Missing value imputation** : Replace missing values, possibly using other known values\n",
    "2. **Feature engineering** : Define useful features from available ones. \n",
    "3. **Feature selection** : Some features might be useless or give wrong indications to the model, we might need to remove some features.\n",
    "\n",
    "Let's start by importing our data, then develop each of the above parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set features\n",
    "training_set_features <- read.csv(\"training_set_features.csv\", stringsAsFactors = T)\n",
    "dim(training_set_features)\n",
    "\n",
    "# Test set features\n",
    "test_set_features <- read.csv(\"test_set_features.csv\", stringsAsFactors = T)\n",
    "dim(test_set_features)\n",
    "\n",
    "# Training set labels\n",
    "training_set_labels <- read.csv(\"training_set_labels.csv\", stringsAsFactors = T)\n",
    "dim(training_set_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training features set and the training labels set has the same amount of lines, this is a first good sign because it means that we have an \"answer\" for every training line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We summarize our data before doing any work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(training_set_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have many missing values, in most features. We can compare the number of lines left if we remove any line containing any missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First method\n",
    "cat(\"Training set : \", dim(training_set_features)[1], \"->\", dim(na.omit(training_set_features))[1], \"\\n\")\n",
    "cat(\"Test set     : \", dim(test_set_features)[1], \"->\", dim(na.omit(test_set_features))[1], \"\\n\")\n",
    "cat(\"Training labs: \", dim(training_set_labels)[1], \"->\", dim(na.omit(training_set_labels))[1])\n",
    "\n",
    "# Second method (when TRUE, there is at least one missing value in the column)\n",
    "# apply(is.na(training_set_features),2,any)\n",
    "# apply(is.na(test_set_features),2,any)\n",
    "# apply(is.na(training_set_labels),2,any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least no line from the training labels misses any value, we can thus use every entry from the training set for both targets.\n",
    "Counting the number of missing value per feature allows us to see if some of the could be useless. The health insurance line is the emptiest (almost half of the lines miss this data) but by intuition this might be a huge factor in the vaccination decision so we keep it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut aussi regarder si certaines colonnes n'ont vraiment quasi aucune valeur, dans ce cas, ça vaut pas vraiment la peine de garder\n",
    "print(sapply(training_set_features, function(x) sum(is.na(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A partir d'ici j'ai juste copié-collé ce que Jeanne avait fait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Je le fais d'abord avec le training_set_features mais il faudra appliquer pareil au test_set_features je pense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(training_set_features)\n",
    "head(training_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(training_set_features[1,],class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(training_set_features)\n",
    "factor_variables<-which(sapply(training_set_features[1,],class)==\"factor\")\n",
    "factor_variables\n",
    "data_factor<-training_set_features[,factor_variables]\n",
    "dim(data_factor)\n",
    "data_preprocessed<-training_set_features[,-factor_variables]\n",
    "head(data_preprocessed)\n",
    "dim(data_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramme des différentes données déjà traitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode pas ouf mais OK\n",
    "\n",
    "# install.packages(\"Hmisc\")\n",
    "# library(Hmisc)\n",
    "# hist.data.frame(na.omit(training_set_features[, -c(1)]))\n",
    "# hist(training_set_features[33])\n",
    "\n",
    "# Meilleure méthode mais on n'a pas encore remplacé les missing values\n",
    "library(reshape2)\n",
    "library(ggplot2)\n",
    "d <- melt(as.data.frame(data_preprocessed[,-c(1)]))\n",
    "ggplot(d,aes(x = value)) + \n",
    "    facet_wrap(~variable,scales = \"free_x\") + \n",
    "    geom_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose several variables that should influence the output variables in our opinion and according to the scientific articles we have read.\n",
    "### Pas hésiter à les mettre quand on en aura hihi\n",
    "Mais selon moi, \"marital_status\", \"rent_or_own\", \"education\" et \"employment_status\" devraient pas trop influencer. Et \"employment_industry\" & \"employment_occupation\" sont des random short strings hyper nombreux donc pas convaincue que ça soit très utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need dummies package to transform string values with one-hot-encoding.\n",
    "# install.packages('dummies')\n",
    "library(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep only some factor variables\n",
    "variables_to_keep<-c(\"age_group\",\"race\",\"sex\",\"income_poverty\",\"hhs_geo_region\",\"census_msa\")\n",
    "data_factor_onehot <- dummy.data.frame(data_factor[,variables_to_keep], sep=\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(data_factor_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factor_onehot[1:2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed_extended<-cbind(data_preprocessed,data_factor_onehot)\n",
    "dim(data_preprocessed_extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut passer au traitement des missing values. Pour le moment, j'implémente la même chose que dans les TP (on remplace par la mean value) mais on peut 100% faire plus de recherches et remplacer par autre chose. J'ai vu que la median value était souvent utilisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_na_with_mean_value<-function(vec) {\n",
    "    mean_vec<-mean(vec,na.rm=T)\n",
    "    vec[is.na(vec)]<-mean_vec\n",
    "    vec\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed_extended<-data.frame(apply(data_preprocessed_extended,2,replace_na_with_mean_value))\n",
    "summary(data_preprocessed_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(na.omit(data_preprocessed_extended))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade, on a un set de data avec seulement des variables numériques qui ont du sens d'être là (selon moi hihi) et plus de missing values donc il manque surtout feature engineering. On pourra ensuite faire feature selection avec une mesure de la corrélation entre input et output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "## Model 1\n",
    "We are going to use the simple linear model first to see what kind of accuracy we can get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D'abord on met tout ensemble pour avoir un tableau avec features+labels\n",
    "label_1 <- training_set_labels[,\"h1n1_vaccine\"]\n",
    "label_2 <- training_set_labels[, \"seasonal_vaccine\"]\n",
    "labels <- cbind(h1n1_vaccine=label_1, seasonal_vaccine=label_2)\n",
    "total_train_set <- cbind(data_preprocessed_extended,labels)\n",
    "#total_train_set\n",
    "# On peut lancer le modèle\n",
    "model <- lm(labels~., total_train_set)\n",
    "#model_bis <- lm(h1n1_vaccine+seasonal_vaccine~.-race_White-sex_Male, total_train_set)\n",
    "#model1 <- lm(h1n1_vaccine~.-seasonal_vaccine, total_train_set)\n",
    "#model2 <- lm(seasonal_vaccine~.-h1n1_vaccine, total_train_set)\n",
    "#Y_hat<- predict(model,data_preprocessed_extended)\n",
    "#empirical_error<-mean((Y_hat-labels)^2)\n",
    "#print(paste(\"Empirical error=\",round(empirical_error,digits=4)))\n",
    "#summary(model) #A décommentariser!\n",
    "Y_pred <- predict(model,subset(total_train_set, select=-c(h1n1_vaccine+seasonal_vaccine)))\n",
    "#print(\"La prédiction est :\")\n",
    "#Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_threshold <- ifelse(Y_pred > 0.5, 1, 0)\n",
    "confusion_matrix <- table(Y_threshold,labels)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je pense que c'est bon pour le linear model, il faudra tester avec le set de test et surtout les bonnes features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "Then we use the randomforest package to make another model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"randomForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"randomForest\")\n",
    "n_trees <- 5\n",
    "n_trees\n",
    "accuracy_vec <- array(0,n_trees)\n",
    "accuracy_vec\n",
    "\n",
    "vaccine_idx <- sample(1:nrow(total_train_set))\n",
    "half_split <- floor(nrow(total_train_set)/2)\n",
    "half_split\n",
    "\n",
    "for (i in 1:n_trees){ print(i)\n",
    "    #3.1 Take the first half of the dataset as a training data set\n",
    "    train_data <- total_train_set[vaccine_idx[1:half_split],]\n",
    "\n",
    "    #3.2 Take the second half of the dataset as a hold out or test data set\n",
    "    test_data <- total_train_set[vaccine_idx[(half_split+1):nrow(total_train_set)],]\n",
    "    \n",
    "    model <- randomForest(x=train_data[,-c(labels)],\n",
    "                          y=as.factor(train_data[,c(labels)]),\n",
    "                          xtest=test_data[,-c(labels)],\n",
    "                          ytest=as.factor(test_data[,c(labels)]),\n",
    "                          ntree=i)\n",
    "    \n",
    "    accuracy_vec[i] = (model$test$confusion[1,1]+model$test$confusion[2,2])/sum(model$test$confusion)\n",
    "    }\n",
    "\n",
    "plot(accuracy_vec,main = \"Number of trees influence\",xlab = \"Nbr of trees\",ylab = \"Classification rate\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne pas décommenter car pas sûre que ça fonctionne\n",
    "#library(\"randomForest\")\n",
    "# Create the forest.\n",
    "#output.forest <- randomForest(formula=h1n1_vaccine+seasonal_vaccine~., data=total_train_set)\n",
    "\n",
    "# View the forest results.\n",
    "#print(output.forest) \n",
    "\n",
    "# Importance of each predictor.\n",
    "#print(importance(fit,type = 2)) \n",
    "#help randomForest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "\n",
    "#### Example of simple equation\n",
    "\\begin{equation}\n",
    "e = mc^2\n",
    "\\end{equation}\n",
    "\n",
    "#### Example of matrix equation - Cross product formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{V}_1 \\times \\mathbf{V}_2 =  \\begin{vmatrix}\n",
    "\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n",
    "\\frac{\\partial X}{\\partial u} &  \\frac{\\partial Y}{\\partial u} & 0 \\\\\n",
    "\\frac{\\partial X}{\\partial v} &  \\frac{\\partial Y}{\\partial v} & 0\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "#### Example of multiline equation - The Lorenz Equations:\n",
    "\n",
    "\\begin{align}\n",
    "\\dot{x} & = \\sigma(y-x) \\\\\n",
    "\\dot{y} & = \\rho x - y - xz \\\\\n",
    "\\dot{z} & = -\\beta z + xy\n",
    "\\end{align}\n",
    "\n",
    "#### Example of Markdown Table:\n",
    "\n",
    "| This | is   |\n",
    "|------|------|\n",
    "|   a  | table|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative models\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
